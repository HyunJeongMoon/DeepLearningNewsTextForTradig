{"cells":[{"cell_type":"markdown","metadata":{"id":"bx5_A4eT3-d8"},"source":["#* 3 종목, 9개 모델 학습 테스트\n","- 최종 선정 학습 모델: LSTM_128\n","- 3개 클러스터 대상  3 종목 테스트에서 가장 정확도가 높은 모델 선정\n","- cluster 2:  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_zXfG6Nt6MPb"},"outputs":[],"source":["# 코랩에서 실행시 pyMySQL을 먼저 설치해야 한다\n","#!pip install JPype1\n","#!pip install konlpy\n","#!pip install pyMySQL\n","#!pip install -U finance-datareader\n","#!pip install attention"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":10666,"status":"ok","timestamp":1709024821727,"user":{"displayName":"hyun","userId":"14298231370728061195"},"user_tz":-540},"id":"ceB0tUnz5X34"},"outputs":[],"source":["#-- Import packages --\n","import os\n","import numpy as np\n","import pandas as pd\n","#import FinanceDataReader as fdr\n","import sqlite3\n","from sqlalchemy import create_engine\n","#import konlpy\n","#from konlpy.tag import Okt\n","import pymysql\n","from os import replace\n","import requests\n","from bs4 import BeautifulSoup\n","from tqdm.notebook import tqdm\n","from datetime import datetime\n","from sqlalchemy import create_engine\n","import ast\n","from os import replace\n","import pickle\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.layers import TextVectorization\n","import re\n","import string\n","import itertools\n","import warnings\n","warnings.filterwarnings('ignore')\n","#-- Import packages :END --"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1709024821727,"user":{"displayName":"hyun","userId":"14298231370728061195"},"user_tz":-540},"id":"EsCSKYw9qBkM"},"outputs":[],"source":["#-- load_data_il_B() -----------------------------------------------------\n","# 일기준 데이터 로더: Binary 1/0 (U/D) 가져오기\n","def load_data_il_B(stock_code):\n","  # 실행확인을 위한 화면 출력\n","  #print(stock_code)\n","  # DB 연결 준비\n","  conn = pymysql.connect(host= '호스트이름', port = 포트번호, user=\"아이디\", password=\"패스워드\", db=\"DB이름\", charset = 'utf8')\n","  # DB에서 종목 데이터  추출위한 sql 준비\n","  sql = f\"SELECT x.* FROM {stock_code}_il_dataset x\"\n","  # DB 검색결과를 dataframe에 저장\n","  il_data_df = pd.read_sql_query(sql, conn)\n","  # 컬럼 이름 정리\n","  il_data_df.columns= ['text','label']\n","  # 레이블 'H' 는 빼기, Up/Down만 남기기\n","  il_data_df = il_data_df[il_data_df['label'] != 'H']\n","  # 레이블 숫자로 변환 return\n","  il_data_df['label'] = il_data_df['label'].replace({'U': 1, 'D': 0})\n","  # DB close\n","  conn.close()\n","  # df 반환\n","  return il_data_df\n","#-- load_data_il_B() : END--------------------------------------------\n","\n","#-- load_data_il() ---------------------------------------------------\n","# 일기준 데이터 로더: Ternary U/H/D 가져오기\n","def load_data_il(stock_code):\n","  # 실행확인을 위한 화면 출력\n","  #print(stock_code)\n","  # DB 연결 준비\n","  conn = pymysql.connect(host= '호스트이름', port = 포트번호, user=\"아이디\", password=\"패스워드\", db=\"DB이름\", charset = 'utf8')\n","  # DB에서 종목 데이터  추출위한 sql 준비\n","  sql = f\"SELECT x.* FROM {stock_code}_il_dataset x\"\n","  # DB 검색결과를 dataframe에 저장\n","  il_data_df = pd.read_sql_query(sql, conn)\n","  # 컬럼 이름 정리\n","  il_data_df.columns= ['text','label']\n","  # 레이블 숫자로 변환 return\n","  #il_data_df['label'] = il_data_df['label'].replace({'U': 3, 'H': 2, 'D': 1})\n","  # DB close\n","  conn.close()\n","  # df 반환\n","  return il_data_df\n","#-- load_data_il() :End-----------------------------------------------\n","\n","#-- load_data_bun_B() ------------------------------------------------\n","# 분기준 데이터 로더: Binary 1/0 (U/D) 가져오기\n","def load_data_bun_B(stock_code):\n","  # 실행확인을 위한 화면 출력\n","  #print(stock_code)\n","  # DB 연결 준비\n","  conn = pymysql.connect(host= '호스트이름', port = 포트번호, user=\"아이디\", password=\"패스워드\", db=\"DB이름\", charset = 'utf8')\n","  # DB에서 종목 데이터  추출위한 sql 준비\n","  sql = f\"SELECT x.* FROM {stock_code}_bun_dataset x\"\n","  # DB 검색결과를 dataframe에 저장\n","  il_data_df = pd.read_sql_query(sql, conn)\n","  # 컬럼 이름 정리\n","  il_data_df.columns= ['text','label']\n","  # Null 있는 행 삭제\n","  il_data_df = il_data_df.dropna()\n","  # 컬럼 이름 정리\n","  il_data_df.columns= ['text','label']\n","  # 레이블 'H' 는 빼기, Up/Down만 남기기\n","  il_data_df = il_data_df[il_data_df['label'] != 'H']\n","  # 레이블 숫자로 변환 return\n","  il_data_df['label'] = il_data_df['label'].replace({'U': 1, 'D': 0})\n","  # DB close\n","  conn.close()\n","  # df 반환\n","  return il_data_df\n","#-- load_data_bun_B() :End--------------------------------------------\n","\n","#-- load_data_bun() --------------------------------------------------\n","# 분기준 데이터 로더: Ternary 2/1/0(U/H/D) 가져오기\n","def load_data_bun(stock_code):\n","  # 실행확인을 위한 화면 출력\n","  #print(stock_code)\n","  # DB 연결 준비\n","  conn = pymysql.connect(host= '호스트이름', port = 포트번호, user=\"아이디\", password=\"패스워드\", db=\"DB이름\", charset = 'utf8')\n","  # DB에서 종목 데이터  추출위한 sql 준비\n","  sql = f\"SELECT x.* FROM {stock_code}_bun_dataset x\"\n","  # DB 검색결과를 dataframe에 저장\n","  il_data_df = pd.read_sql_query(sql, conn)\n","  # 컬럼 이름 정리\n","  il_data_df.columns= ['text','label']\n","  # Null 있는 행 삭제\n","  il_data_df = il_data_df.dropna()\n","  # 레이블 숫자로 변환 return\n","  il_data_df['label'] = il_data_df['label'].replace({'U': 2, 'H': 1, 'D': 0})\n","  # DB close\n","  conn.close()\n","  # df 반환\n","  return il_data_df\n","#-- load_data_bun() :End----------------------------------------------\n","#---------------------------------------------------------------------"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8897,"status":"ok","timestamp":1709024830619,"user":{"displayName":"hyun","userId":"14298231370728061195"},"user_tz":-540},"id":"RNA4AjPUtVYe","outputId":"6e1613eb-61b4-4480-d6b2-392f7943068b"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                                text  label\n","0  [ 18 179 122 9 3749 2377 52 241 3943 1 18 415 ...      2\n","1  [ 743 558 11 1 908 1 12992 581 580 25 91 245 2...      2\n","2  [ 10 187 589 1123 6 267 29 54 2 3004 44 2459 1...      2\n","3 카테고리\n","104288 학습용 뉴스 기사\n","26072 테스트용 뉴스 기사\n","[18, 179, 122, 9, 3749, 2377, 52, 241, 3943, 1, 18, 415, 3741, 1, 179, 122]\n","2\n"]}],"source":["# LSTM+CNN+Attention 카테고리 분류\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation \\\n","                                  , Embedding, LSTM, Conv1D, MaxPooling1D\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.preprocessing import sequence\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from attention import Attention\n","from sklearn.model_selection import train_test_split\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","# 분석대상 종목 코드\n","stock_code = '005930'\n","# data load\n","bun_data_df = load_data_bun(stock_code)\n","# 실행 확인을 위한 화면 출력\n","print(bun_data_df.head(3))\n","\n","# data preparation: 데이터X,레이블y 분리\n","X = bun_data_df['text']\n","y = bun_data_df['label']\n","\n","# 문자열 형식인 데이터X를 리스트로 변환\n","X= X.apply(lambda x: [int(i) for i in x.strip('[]').split()])\n","\n","# data set separation: 학습셋, 테스트셋 분리\n","X_train, X_test,y_train, y_test = train_test_split(X,y,test_size=0.2, shuffle=True)\n","\n","# data print, data 확인\n","# 레이블 카테고리 갯수 계산\n","category = np.max(y_train) + 1\n","# 실행 확인을 위한 화면 출력\n","print(category,'카테고리')\n","print(len(X_train),'학습용 뉴스 기사')\n","print(len(X_test), '테스트용 뉴스 기사')\n","print(X[0])\n","print(y[0])\n","\n","# pad_sequence 단어 수 맞춰주기\n","# 한 기사당 단어 갯수 제한\n","sequence_length = 50\n","X_train = sequence.pad_sequences(X_train, maxlen= sequence_length)\n","X_test = sequence.pad_sequences(X_test,maxlen=sequence_length)\n","\n","# ** 레이블이 1/0 아닌 경우: 원핫 인코딩 처리\n","y_train = to_categorical(y_train)\n","y_test = to_categorical(y_test)\n","\n","# model creation\n","model = Sequential()\n","model.add(Embedding(20000,sequence_length))\n","model.add(Dropout(0.5))\n","model.add(LSTM(64, return_sequences=True))\n","model.add(Attention())\n","model.add(Dense(category, activation='softmax'))    # Output: ternary 이상인 경우\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":141837,"status":"ok","timestamp":1708911051208,"user":{"displayName":"hyun","userId":"14298231370728061195"},"user_tz":-540},"id":"TZ72plQA2IrC","outputId":"75e5bbea-b5be-4142-b446-19d5eadb5ae0"},"outputs":[{"name":"stdout","output_type":"stream","text":["** LSTM **\n","815/815 [==============================] - 7s 8ms/step - loss: 0.5656 - accuracy: 0.8007\n","Model: LSTM_sokbo_034950_30.hdf5, Test Accuracy: 0.8007\n","\n","815/815 [==============================] - 6s 7ms/step - loss: 0.6281 - accuracy: 0.7823\n","Model: LSTM_sokbo_034950_31.hdf5, Test Accuracy: 0.7823\n","\n","815/815 [==============================] - 7s 8ms/step - loss: 0.5796 - accuracy: 0.7941\n","Model: LSTM_sokbo_034950_32.hdf5, Test Accuracy: 0.7941\n","\n","** LSTM_CNN **\n","815/815 [==============================] - 7s 8ms/step - loss: 0.5859 - accuracy: 0.7918\n","Model: LSTMCNN_sokbo_034950_32.hdf5, Test Accuracy: 0.7918\n","\n","815/815 [==============================] - 4s 4ms/step - loss: 0.5884 - accuracy: 0.7915\n","Model: LSTMCNN_sokbo_034950_33.hdf5, Test Accuracy: 0.7915\n","\n","815/815 [==============================] - 13s 14ms/step - loss: 0.5324 - accuracy: 0.8069\n","Model: LSTMCNN_sokbo_034950_34.hdf5, Test Accuracy: 0.8069\n","\n","** LSTM_CNN_Attention **\n","815/815 [==============================] - 11s 13ms/step - loss: 0.5370 - accuracy: 0.8047\n","Model: LSTMCNNATT_sokbo_034950_30.hdf5, Test Accuracy: 0.8047\n","\n","815/815 [==============================] - 10s 11ms/step - loss: 0.5906 - accuracy: 0.7876\n","Model: LSTMCNNATT_sokbo_034950_31.hdf5, Test Accuracy: 0.7876\n","\n","815/815 [==============================] - 11s 13ms/step - loss: 0.3735 - accuracy: 0.8727\n","Model: LSTMCNNATT_sokbo_034950_32.hdf5, Test Accuracy: 0.8727\n","\n","** Transformer **\n","815/815 [==============================] - 11s 13ms/step - loss: 0.2025 - accuracy: 0.9458\n","Model: TFMR_sokbo_034950_30.hdf5, Test Accuracy: 0.9458\n","\n","815/815 [==============================] - 10s 12ms/step - loss: 0.1086 - accuracy: 0.9715\n","Model: TFMR_sokbo_034950_31.hdf5, Test Accuracy: 0.9715\n","\n","815/815 [==============================] - 10s 11ms/step - loss: 0.0630 - accuracy: 0.9832\n","Model: TFMR_sokbo_034950_32.hdf5, Test Accuracy: 0.9832\n","\n"]}],"source":["from tensorflow.keras.models import Model, Sequential,load_model\n","\n","print('** LSTM **')\n","#-----------------------------------------------------------------------------------\n","model_name = 'LSTM_sokbo_034950_30.hdf5'\n","model = load_model(filepath=f'./data/model/{model_name}')\n","print(f'Model: {model_name}, Test Accuracy: {model.evaluate(X_test,y_test)[1]:.4f}\\n')\n","#-----------------------------------------------------------------------------------\n","#-----------------------------------------------------------------------------------\n","model_name = 'LSTM_sokbo_034950_31.hdf5'\n","model = load_model(filepath=f'./data/model/{model_name}')\n","print(f'Model: {model_name}, Test Accuracy: {model.evaluate(X_test,y_test)[1]:.4f}\\n')\n","#-----------------------------------------------------------------------------------\n","#-----------------------------------------------------------------------------------\n","model_name = 'LSTM_sokbo_034950_32.hdf5'\n","model = load_model(filepath=f'./data/model/{model_name}')\n","print(f'Model: {model_name}, Test Accuracy: {model.evaluate(X_test,y_test)[1]:.4f}\\n')\n","#-----------------------------------------------------------------------------------\n","print('** LSTM_CNN **')\n","#-----------------------------------------------------------------------------------\n","model_name = 'LSTMCNN_sokbo_034950_32.hdf5'\n","model = load_model(filepath=f'./data/model/{model_name}')\n","print(f'Model: {model_name}, Test Accuracy: {model.evaluate(X_test,y_test)[1]:.4f}\\n')\n","#-----------------------------------------------------------------------------------\n","#-----------------------------------------------------------------------------------\n","model_name = 'LSTMCNN_sokbo_034950_33.hdf5'\n","model = load_model(filepath=f'./data/model/{model_name}')\n","print(f'Model: {model_name}, Test Accuracy: {model.evaluate(X_test,y_test)[1]:.4f}\\n')\n","#-----------------------------------------------------------------------------------\n","#-----------------------------------------------------------------------------------\n","model_name = 'LSTMCNN_sokbo_034950_34.hdf5'\n","model = load_model(filepath=f'./data/model/{model_name}')\n","print(f'Model: {model_name}, Test Accuracy: {model.evaluate(X_test,y_test)[1]:.4f}\\n')\n","#-----------------------------------------------------------------------------------\n","print('** LSTM_CNN_Attention **')\n","#-----------------------------------------------------------------------------------\n","model_name = 'LSTMCNNATT_sokbo_034950_30.hdf5'\n","model = load_model(filepath=f'./data/model/{model_name}',custom_objects={'Attention': Attention})\n","print(f'Model: {model_name}, Test Accuracy: {model.evaluate(X_test,y_test)[1]:.4f}\\n')\n","#-----------------------------------------------------------------------------------\n","#-----------------------------------------------------------------------------------\n","model_name = 'LSTMCNNATT_sokbo_034950_31.hdf5'\n","model = load_model(filepath=f'./data/model/{model_name}',custom_objects={'Attention': Attention})\n","print(f'Model: {model_name}, Test Accuracy: {model.evaluate(X_test,y_test)[1]:.4f}\\n')\n","#-----------------------------------------------------------------------------------\n","#-----------------------------------------------------------------------------------\n","model_name = 'LSTMCNNATT_sokbo_034950_32.hdf5'\n","model = load_model(filepath=f'./data/model/{model_name}',custom_objects={'Attention': Attention})\n","print(f'Model: {model_name}, Test Accuracy: {model.evaluate(X_test,y_test)[1]:.4f}\\n')\n","#-----------------------------------------------------------------------------------\n","\n","print('** Transformer **')\n","#-----------------------------------------------------------------------------------\n","model_name = 'TFMR_sokbo_034950_30.hdf5'\n","model = load_model(filepath=f'./data/model/{model_name}',custom_objects={'Attention': Attention})\n","print(f'Model: {model_name}, Test Accuracy: {model.evaluate(X_test,y_test)[1]:.4f}\\n')\n","#-----------------------------------------------------------------------------------\n","#-----------------------------------------------------------------------------------\n","model_name = 'TFMR_sokbo_034950_31.hdf5'\n","model = load_model(filepath=f'./data/model/{model_name}',custom_objects={'Attention': Attention})\n","print(f'Model: {model_name}, Test Accuracy: {model.evaluate(X_test,y_test)[1]:.4f}\\n')\n","#-----------------------------------------------------------------------------------\n","#-----------------------------------------------------------------------------------\n","model_name = 'TFMR_sokbo_034950_32.hdf5'\n","model = load_model(filepath=f'./data/model/{model_name}',custom_objects={'Attention': Attention})\n","print(f'Model: {model_name}, Test Accuracy: {model.evaluate(X_test,y_test)[1]:.4f}\\n')\n","#-----------------------------------------------------------------------------------"]},{"cell_type":"markdown","metadata":{"id":"bhwB49hrvZn6"},"source":["# '005930' 종목"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ZhboOnthSQ9"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Input, Embedding, GlobalAveragePooling1D, Dense, Dropout, LayerNormalization\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import CategoricalCrossentropy\n","from tensorflow.keras.metrics import CategoricalAccuracy\n","import numpy as np\n","\n","class MultiHeadSelfAttention(tf.keras.layers.Layer):\n","    def __init__(self, embed_dim, num_heads=8):\n","        super(MultiHeadSelfAttention, self).__init__()\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads\n","        self.projection_dim = embed_dim // num_heads\n","        self.query_dense = tf.keras.layers.Dense(embed_dim)\n","        self.key_dense = tf.keras.layers.Dense(embed_dim)\n","        self.value_dense = tf.keras.layers.Dense(embed_dim)\n","        self.combine_heads = tf.keras.layers.Dense(embed_dim)\n","\n","    def attention(self, query, key, value):\n","        score = tf.matmul(query, key, transpose_b=True)\n","        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n","        scaled_score = score / tf.math.sqrt(dim_key)\n","        weights = tf.nn.softmax(scaled_score, axis=-1)\n","        output = tf.matmul(weights, value)\n","        return output\n","\n","    def separate_heads(self, x, batch_size):\n","        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","\n","    def call(self, inputs):\n","        batch_size = tf.shape(inputs)[0]\n","        query = self.query_dense(inputs)\n","        key = self.key_dense(inputs)\n","        value = self.value_dense(inputs)\n","        query = self.separate_heads(query, batch_size)\n","        key = self.separate_heads(key, batch_size)\n","        value = self.separate_heads(value, batch_size)\n","        attention = self.attention(query, key, value)\n","        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n","        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n","        output = self.combine_heads(concat_attention)\n","        return output\n","\n","def transformer_encoder(inputs, embed_dim, num_heads, ff_dim, rate=0.1):\n","    attn = MultiHeadSelfAttention(embed_dim, num_heads)(inputs)\n","    attn = Dropout(rate)(attn)\n","    out = LayerNormalization(epsilon=1e-6)(inputs + attn)\n","    ffn = tf.keras.Sequential(\n","        [Dense(ff_dim, activation=\"relu\"), Dense(embed_dim),]\n","    )\n","    ffn_output = ffn(out)\n","    ffn_output = Dropout(rate)(ffn_output)\n","    out = LayerNormalization(epsilon=1e-6)(out + ffn_output)\n","    return out\n","\n","# Define Transformer model\n","def build_transformer_model(max_seq_length, vocab_size, embed_dim, num_heads, ff_dim, num_classes):\n","    inputs = Input(shape=(max_seq_length,))\n","    embedding_layer = Embedding(input_dim=vocab_size, output_dim=embed_dim)(inputs)\n","    transformer_block = transformer_encoder(embedding_layer, embed_dim, num_heads, ff_dim)\n","    pool = GlobalAveragePooling1D()(transformer_block)\n","    outputs = Dense(num_classes, activation='softmax')(pool)\n","    model = Model(inputs=inputs, outputs=outputs)\n","    return model\n","\n","# Define hyperparameters\n","max_seq_length = 50\n","vocab_size = 20000\n","embed_dim = 50\n","num_heads = 5\n","ff_dim = 128\n","num_classes = 3\n","learning_rate = 1e-4\n","\n","# Build and compile the model\n","transformer_model = build_transformer_model(max_seq_length, vocab_size, embed_dim, num_heads, ff_dim, num_classes)\n","optimizer = Adam(learning_rate=learning_rate)\n","loss_fn = CategoricalCrossentropy()\n","accuracy_metric = CategoricalAccuracy()\n","\n","transformer_model.compile(optimizer=optimizer, loss=loss_fn, metrics=[accuracy_metric])\n","\n","# Assuming y_train is one-hot encoded\n","transformer_model.fit(X_train, y_train, batch_size=32, epochs=10, validation_split=0.2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7095,"status":"ok","timestamp":1709000786452,"user":{"displayName":"hyun","userId":"14298231370728061195"},"user_tz":-540},"id":"fmwokCLAR5Tu","outputId":"0c8625c4-11f7-47da-eecc-bd642a1cbe35"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                                text  label\n","0  [ 18 179 122 9 3749 2377 52 241 3943 1 18 415 ...      2\n","1  [ 743 558 11 1 908 1 12992 581 580 25 91 245 2...      2\n","2  [ 10 187 589 1123 6 267 29 54 2 3004 44 2459 1...      2\n","3 카테고리\n","104288 학습용 뉴스 기사\n","26072 테스트용 뉴스 기사\n"]}],"source":["import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Embedding, GlobalAveragePooling1D, Dense, Dropout, LayerNormalization\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import SparseCategoricalCrossentropy\n","from tensorflow.keras.metrics import SparseCategoricalAccuracy\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing import sequence\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Embedding, LSTM, Conv1D, MaxPooling1D\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from attention import Attention\n","\n","class MultiHeadSelfAttention(tf.keras.layers.Layer):\n","    def __init__(self, embed_dim, num_heads=8):\n","        super(MultiHeadSelfAttention, self).__init__()\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads\n","        self.projection_dim = embed_dim // num_heads\n","        self.query_dense = tf.keras.layers.Dense(embed_dim)\n","        self.key_dense = tf.keras.layers.Dense(embed_dim)\n","        self.value_dense = tf.keras.layers.Dense(embed_dim)\n","        self.combine_heads = tf.keras.layers.Dense(embed_dim)\n","\n","    def attention(self, query, key, value):\n","        score = tf.matmul(query, key, transpose_b=True)\n","        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n","        scaled_score = score / tf.math.sqrt(dim_key)\n","        weights = tf.nn.softmax(scaled_score, axis=-1)\n","        output = tf.matmul(weights, value)\n","        return output\n","\n","    def separate_heads(self, x, batch_size):\n","        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","    def call(self, inputs):\n","        batch_size = tf.shape(inputs)[0]\n","        query = self.query_dense(inputs)\n","        key = self.key_dense(inputs)\n","        value = self.value_dense(inputs)\n","        query = self.separate_heads(query, batch_size)\n","        key = self.separate_heads(key, batch_size)\n","        value = self.separate_heads(value, batch_size)\n","        attention = self.attention(query, key, value)\n","        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n","        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n","        output = self.combine_heads(concat_attention)\n","        return output\n","\n","def transformer_encoder(inputs, embed_dim, num_heads, ff_dim, rate=0.1):\n","    attn = MultiHeadSelfAttention(embed_dim, num_heads)(inputs)\n","    attn = Dropout(rate)(attn)\n","    out = LayerNormalization(epsilon=1e-6)(inputs + attn)\n","    ffn = tf.keras.Sequential(\n","        [Dense(ff_dim, activation=\"relu\"), Dense(embed_dim),]\n","    )\n","    ffn_output = ffn(out)\n","    ffn_output = Dropout(rate)(ffn_output)\n","    out = LayerNormalization(epsilon=1e-6)(out + ffn_output)\n","    return out\n","\n","# Define Transformer model\n","def build_transformer_model(max_seq_length, vocab_size, embed_dim, num_heads, ff_dim, num_classes):\n","    inputs = Input(shape=(max_seq_length,))\n","    embedding_layer = Embedding(input_dim=vocab_size, output_dim=embed_dim)(inputs)\n","    transformer_block = transformer_encoder(embedding_layer, embed_dim, num_heads, ff_dim)\n","    pool = GlobalAveragePooling1D()(transformer_block)\n","    outputs = Dense(num_classes, activation='softmax')(pool)\n","    model = Model(inputs=inputs, outputs=outputs)\n","    return model\n","\n","# Define hyperparameters\n","max_seq_length = 50\n","vocab_size = 20000\n","embed_dim = 50\n","num_heads = 5\n","ff_dim = 128\n","num_classes = 3\n","learning_rate = 1e-4\n","\n","# Build and compile the model\n","transformer_model = build_transformer_model(max_seq_length, vocab_size, embed_dim, num_heads, ff_dim, num_classes)\n","optimizer = Adam(learning_rate=learning_rate)\n","loss_fn = SparseCategoricalCrossentropy()\n","accuracy_metric = SparseCategoricalAccuracy()\n","\n","transformer_model.compile(optimizer=optimizer, loss=loss_fn, metrics=[accuracy_metric])\n","# Print model summary\n","#transformer_model.summary()\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","# 분석대상 종목 코드\n","stock_code = '005930'\n","# data load\n","bun_data_df = load_data_bun(stock_code)\n","# 실행 확인을 위한 화면 출력\n","print(bun_data_df.head(3))\n","# data preparation: 데이터X,레이블y 분리\n","X = bun_data_df['text']\n","y = bun_data_df['label']\n","\n","# 문자열 형식인 데이터X를 리스트로 변환\n","X= X.apply(lambda x: [int(i) for i in x.strip('[]').split()])\n","# data set separation: 학습셋, 테스트셋 분리\n","X_train, X_test,y_train, y_test = train_test_split(X,y,test_size=0.2, shuffle=True)\n","# data print, data 확인\n","# 레이블 카테고리 갯수 계산\n","category = np.max(y_train) + 1\n","# 실행 확인을 위한 화면 출력\n","print(category,'카테고리')\n","print(len(X_train),'학습용 뉴스 기사')\n","print(len(X_test), '테스트용 뉴스 기사')\n","#print(X_train.at[0])\n","# pad_sequence 단어 수 맞춰주기\n","# 한 기사당 단어 갯수 제한\n","sequence_length = 50\n","X_train = sequence.pad_sequences(X_train, maxlen= sequence_length)\n","X_test = sequence.pad_sequences(X_test,maxlen=sequence_length)\n","\n","# ** 레이블이 1/0 아닌 경우: 원핫 인코딩 처리\n","# y_train = to_categorical(y_train)\n","# y_test = to_categorical(y_test)\n","\n","# 조기중단 설정, early stopping callback\n","early_stopping_callback = EarlyStopping(\n","    monitor= 'val_loss'                   # 모니터 항목 : val_loss\n","  , patience = 10                          # patiencd    : 5 (epochs)\n",")\n","\n","\n","import tensorflow as tf\n","\n","# 사용자 지정 레이어 등록\n","tf.keras.utils.get_custom_objects()['MultiHeadSelfAttention'] = MultiHeadSelfAttention"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32059,"status":"ok","timestamp":1709000840343,"user":{"displayName":"hyun","userId":"14298231370728061195"},"user_tz":-540},"id":"Kgk3Y_uMvXAN","outputId":"397af5e9-a597-47a4-a07b-e90354232675"},"outputs":[{"name":"stdout","output_type":"stream","text":["** Transformer **\n","815/815 [==============================] - 10s 11ms/step - loss: 0.9869 - sparse_categorical_accuracy: 0.5252\n","Model: TFMR_sokbo_005930_b32_e200_c3.hdf5, Test Accuracy: 0.5252\n","\n","815/815 [==============================] - 10s 12ms/step - loss: 1.0000 - sparse_categorical_accuracy: 0.5144\n","Model: TFMR_sokbo_005930_b64_e200_c3.hdf5, Test Accuracy: 0.5144\n","\n","815/815 [==============================] - 10s 12ms/step - loss: 1.0221 - sparse_categorical_accuracy: 0.4921\n","Model: TFMR_sokbo_005930_b128_e200_c3.hdf5, Test Accuracy: 0.4921\n","\n"]}],"source":["from tensorflow.keras.models import Model, Sequential,load_model\n","\n","# print('** LSTM **')\n","# #-----------------------------------------------------------------------------------\n","# model_name = 'LSTM_sokbo_005930_b32_e200_c32.hdf5'\n","# model = load_model(filepath=f'./data/model/{model_name}')\n","# print(f'Model: {model_name}, Test Accuracy: {model.evaluate(X_test,y_test)[1]:.4f}\\n')\n","# #-----------------------------------------------------------------------------------\n","# #-----------------------------------------------------------------------------------\n","# model_name = 'LSTM_sokbo_005930_b64_e200_c32.hdf5'\n","# model = load_model(filepath=f'./data/model/{model_name}')\n","# print(f'Model: {model_name}, Test Accuracy: {model.evaluate(X_test,y_test)[1]:.4f}\\n')\n","# #-----------------------------------------------------------------------------------\n","# #-----------------------------------------------------------------------------------\n","# model_name = 'LSTM_sokbo_005930_b128_e200_c32.hdf5'\n","# model = load_model(filepath=f'./data/model/{model_name}')\n","# print(f'Model: {model_name}, Test Accuracy: {model.evaluate(X_test,y_test)[1]:.4f}\\n')\n","# #-----------------------------------------------------------------------------------\n","# print('** LSTM_CNN **')\n","# #-----------------------------------------------------------------------------------\n","# model_name = 'LSTMCNN_sokbo_005930_b32_e200_c3.hdf5'\n","# model = load_model(filepath=f'./data/model/{model_name}')\n","# print(f'Model: {model_name}, Test Accuracy: {model.evaluate(X_test,y_test)[1]:.4f}\\n')\n","# #-----------------------------------------------------------------------------------\n","# #-----------------------------------------------------------------------------------\n","# model_name = 'LSTMCNN_sokbo_005930_b64_e200_c3.hdf5'\n","# model = load_model(filepath=f'./data/model/{model_name}')\n","# print(f'Model: {model_name}, Test Accuracy: {model.evaluate(X_test,y_test)[1]:.4f}\\n')\n","# #-----------------------------------------------------------------------------------\n","# #-----------------------------------------------------------------------------------\n","# model_name = 'LSTMCNN_sokbo_005930_b128_e200_c3.hdf5'\n","# model = load_model(filepath=f'./data/model/{model_name}')\n","# print(f'Model: {model_name}, Test Accuracy: {model.evaluate(X_test,y_test)[1]:.4f}\\n')\n","# #-----------------------------------------------------------------------------------\n","# print('** LSTM_CNN_Attention **')\n","# #-----------------------------------------------------------------------------------\n","# model_name = 'LSTMCNNATT_sokbo_005930_b32_e200_c3.hdf5'\n","# model = load_model(filepath=f'./data/model/{model_name}',custom_objects={'Attention': Attention})\n","# print(f'Model: {model_name}, Test Accuracy: {model.evaluate(X_test,y_test)[1]:.4f}\\n')\n","# #-----------------------------------------------------------------------------------\n","# #-----------------------------------------------------------------------------------\n","# model_name = 'LSTMCNNATT_sokbo_005930_b64_e200_c3.hdf5'\n","# model = load_model(filepath=f'./data/model/{model_name}',custom_objects={'Attention': Attention})\n","# print(f'Model: {model_name}, Test Accuracy: {model.evaluate(X_test,y_test)[1]:.4f}\\n')\n","# #-----------------------------------------------------------------------------------\n","# #-----------------------------------------------------------------------------------\n","# model_name = 'LSTMCNNATT_sokbo_005930_b128_e200_c3.hdf5'\n","# model = load_model(filepath=f'./data/model/{model_name}',custom_objects={'Attention': Attention})\n","# print(f'Model: {model_name}, Test Accuracy: {model.evaluate(X_test,y_test)[1]:.4f}\\n')\n","# #-----------------------------------------------------------------------------------\n","\n","print('** Transformer **')\n","#-----------------------------------------------------------------------------------\n","model_name = 'TFMR_sokbo_005930_b32_e200_c3.hdf5'\n","model = load_model(filepath=f'./data/model/{model_name}')#,custom_objects={'Attention': Attention})\n","print(f'Model: {model_name}, Test Accuracy: {model.evaluate(X_test,y_test)[1]:.4f}\\n')\n","#-----------------------------------------------------------------------------------\n","#-----------------------------------------------------------------------------------\n","model_name = 'TFMR_sokbo_005930_b64_e200_c3.hdf5'\n","model = load_model(filepath=f'./data/model/{model_name}',custom_objects={'Attention': Attention})\n","print(f'Model: {model_name}, Test Accuracy: {model.evaluate(X_test,y_test)[1]:.4f}\\n')\n","#-----------------------------------------------------------------------------------\n","#-----------------------------------------------------------------------------------\n","model_name = 'TFMR_sokbo_005930_b128_e200_c3.hdf5'\n","model = load_model(filepath=f'./data/model/{model_name}',custom_objects={'Attention': Attention})\n","print(f'Model: {model_name}, Test Accuracy: {model.evaluate(X_test,y_test)[1]:.4f}\\n')\n","#-----------------------------------------------------------------------------------"]},{"cell_type":"markdown","metadata":{"id":"PYTerwU1z1Ic"},"source":["#* 328380 종목"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7241,"status":"ok","timestamp":1709026053825,"user":{"displayName":"hyun","userId":"14298231370728061195"},"user_tz":-540},"id":"ZkvHXIT3z0ur","outputId":"ed59faaf-9fb8-434f-f406-701ebab610e6"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                                text  label\n","0  [ 18 179 122 9 3749 2377 52 241 3943 1 18 415 ...      2\n","1  [ 743 558 11 1 908 1 12992 581 580 25 91 245 2...      2\n","2  [ 10 187 589 1123 6 267 29 54 2 3004 44 2459 1...      2\n","3 카테고리\n","104308 학습용 뉴스 기사\n","26078 테스트용 뉴스 기사\n","[18, 179, 122, 9, 3749, 2377, 52, 241, 3943, 1, 18, 415, 3741, 1, 179, 122]\n","2\n"]}],"source":["# LSTM+CNN+Attention 카테고리 분류\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation \\\n","                                  , Embedding, LSTM, Conv1D, MaxPooling1D\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.preprocessing import sequence\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from attention import Attention\n","from sklearn.model_selection import train_test_split\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","# 분석대상 종목 코드\n","stock_code = '328380'\n","# data load\n","bun_data_df = load_data_bun(stock_code)\n","# 실행 확인을 위한 화면 출력\n","print(bun_data_df.head(3))\n","\n","# data preparation: 데이터X,레이블y 분리\n","X = bun_data_df['text']\n","y = bun_data_df['label']\n","\n","# 문자열 형식인 데이터X를 리스트로 변환\n","X= X.apply(lambda x: [int(i) for i in x.strip('[]').split()])\n","\n","# data set separation: 학습셋, 테스트셋 분리\n","X_train, X_test,y_train, y_test = train_test_split(X,y,test_size=0.2, shuffle=True)\n","\n","# data print, data 확인\n","# 레이블 카테고리 갯수 계산\n","category = np.max(y_train) + 1\n","# 실행 확인을 위한 화면 출력\n","print(category,'카테고리')\n","print(len(X_train),'학습용 뉴스 기사')\n","print(len(X_test), '테스트용 뉴스 기사')\n","print(X[0])\n","print(y[0])\n","\n","# pad_sequence 단어 수 맞춰주기\n","# 한 기사당 단어 갯수 제한\n","sequence_length = 50\n","X_train = sequence.pad_sequences(X_train, maxlen= sequence_length)\n","X_test = sequence.pad_sequences(X_test,maxlen=sequence_length)\n","\n","# ** 레이블이 1/0 아닌 경우: 원핫 인코딩 처리\n","y_train = to_categorical(y_train)\n","y_test = to_categorical(y_test)\n","\n","# # model creation\n","# model = Sequential()\n","# model.add(Embedding(20000,sequence_length))\n","# model.add(Dropout(0.5))\n","# model.add(LSTM(64, return_sequences=True))\n","# model.add(Attention())\n","# model.add(Dense(category, activation='softmax'))    # Output: ternary 이상인 경우\n"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":84877,"status":"ok","timestamp":1709026142778,"user":{"displayName":"hyun","userId":"14298231370728061195"},"user_tz":-540},"id":"bYBiSD-AOSnc","outputId":"5b617b64-7f48-4484-e2c9-3af32ac6d6a2"},"outputs":[{"name":"stdout","output_type":"stream","text":["** LSTM **\n","815/815 [==============================] - 9s 10ms/step - loss: 1.0108 - accuracy: 0.5010\n","Model: LSTM_sokbo_328380_b32_e200_c32.hdf5, Test Accuracy: 0.50\n","\n","815/815 [==============================] - 9s 10ms/step - loss: 1.0103 - accuracy: 0.4983\n","Model: LSTM_sokbo_328380_b64_e200_c32.hdf5, Test Accuracy: 0.50\n","\n","815/815 [==============================] - 9s 10ms/step - loss: 1.0046 - accuracy: 0.5050\n","Model: LSTM_sokbo_328380_b128_e200_c32.hdf5, Test Accuracy: 0.50\n","\n","** LSTM_CNN **\n","815/815 [==============================] - 5s 6ms/step - loss: 0.9521 - accuracy: 0.5568\n","Model: LSTMCNN_sokbo_328380_b32_e200_c3.hdf5, Test Accuracy: 0.56\n","\n","815/815 [==============================] - 5s 5ms/step - loss: 0.9506 - accuracy: 0.5418\n","Model: LSTMCNN_sokbo_328380_b64_e200_c3.hdf5, Test Accuracy: 0.54\n","\n","815/815 [==============================] - 5s 5ms/step - loss: 0.9545 - accuracy: 0.5560\n","Model: LSTMCNN_sokbo_328380_b128_e200_c3.hdf5, Test Accuracy: 0.56\n","\n","** LSTM_CNN_Attention **\n","815/815 [==============================] - 10s 11ms/step - loss: 0.9466 - accuracy: 0.5426\n","Model: LSTMCNNATT_sokbo_328380_b32_e200_c3.hdf5, Test Accuracy: 0.54\n","\n","815/815 [==============================] - 10s 12ms/step - loss: 0.9485 - accuracy: 0.5460\n","Model: LSTMCNNATT_sokbo_328380_b64_e200_c3.hdf5, Test Accuracy: 0.55\n","\n","815/815 [==============================] - 11s 13ms/step - loss: 0.9507 - accuracy: 0.5467\n","Model: LSTMCNNATT_sokbo_328380_b128_e200_c3.hdf5, Test Accuracy: 0.55\n","\n"]}],"source":["from tensorflow.keras.models import Model, Sequential,load_model\n","\n","print('** LSTM **')\n","#-----------------------------------------------------------------------------------\n","model_name = 'LSTM_sokbo_328380_b32_e200_c32.hdf5'\n","model = load_model(filepath=f'./data/model/{model_name}')\n","print(f'Model: {model_name}, Test Accuracy: {model.evaluate(X_test,y_test)[1]:.2f}\\n')\n","#-----------------------------------------------------------------------------------\n","#-----------------------------------------------------------------------------------\n","model_name = 'LSTM_sokbo_328380_b64_e200_c32.hdf5'\n","model = load_model(filepath=f'./data/model/{model_name}')\n","print(f'Model: {model_name}, Test Accuracy: {model.evaluate(X_test,y_test)[1]:.2f}\\n')\n","#-----------------------------------------------------------------------------------\n","#-----------------------------------------------------------------------------------\n","model_name = 'LSTM_sokbo_328380_b128_e200_c32.hdf5'\n","model = load_model(filepath=f'./data/model/{model_name}')\n","print(f'Model: {model_name}, Test Accuracy: {model.evaluate(X_test,y_test)[1]:.2f}\\n')\n","#-----------------------------------------------------------------------------------\n","print('** LSTM_CNN **')\n","#-----------------------------------------------------------------------------------\n","model_name = 'LSTMCNN_sokbo_328380_b32_e200_c3.hdf5'\n","model = load_model(filepath=f'./data/model/{model_name}')\n","print(f'Model: {model_name}, Test Accuracy: {model.evaluate(X_test,y_test)[1]:.2f}\\n')\n","#-----------------------------------------------------------------------------------\n","#-----------------------------------------------------------------------------------\n","model_name = 'LSTMCNN_sokbo_328380_b64_e200_c3.hdf5'\n","model = load_model(filepath=f'./data/model/{model_name}')\n","print(f'Model: {model_name}, Test Accuracy: {model.evaluate(X_test,y_test)[1]:.2f}\\n')\n","#-----------------------------------------------------------------------------------\n","#-----------------------------------------------------------------------------------\n","model_name = 'LSTMCNN_sokbo_328380_b128_e200_c3.hdf5'\n","model = load_model(filepath=f'./data/model/{model_name}')\n","print(f'Model: {model_name}, Test Accuracy: {model.evaluate(X_test,y_test)[1]:.2f}\\n')\n","#-----------------------------------------------------------------------------------\n","print('** LSTM_CNN_Attention **')\n","#-----------------------------------------------------------------------------------\n","model_name = 'LSTMCNNATT_sokbo_328380_b32_e200_c3.hdf5'\n","model = load_model(filepath=f'./data/model/{model_name}',custom_objects={'Attention': Attention})\n","print(f'Model: {model_name}, Test Accuracy: {model.evaluate(X_test,y_test)[1]:.2f}\\n')\n","#-----------------------------------------------------------------------------------\n","#-----------------------------------------------------------------------------------\n","model_name = 'LSTMCNNATT_sokbo_328380_b64_e200_c3.hdf5'\n","model = load_model(filepath=f'./data/model/{model_name}',custom_objects={'Attention': Attention})\n","print(f'Model: {model_name}, Test Accuracy: {model.evaluate(X_test,y_test)[1]:.2f}\\n')\n","#-----------------------------------------------------------------------------------\n","#-----------------------------------------------------------------------------------\n","model_name = 'LSTMCNNATT_sokbo_328380_b128_e200_c3.hdf5'\n","model = load_model(filepath=f'./data/model/{model_name}',custom_objects={'Attention': Attention})\n","print(f'Model: {model_name}, Test Accuracy: {model.evaluate(X_test,y_test)[1]:.2f}\\n')\n","#-----------------------------------------------------------------------------------\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7452,"status":"ok","timestamp":1709025484442,"user":{"displayName":"hyun","userId":"14298231370728061195"},"user_tz":-540},"id":"6TM3KyLt15uL","outputId":"718879d2-9623-4c88-a331-970c9a839470"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                                text  label\n","0  [ 18 179 122 9 3749 2377 52 241 3943 1 18 415 ...      2\n","1  [ 743 558 11 1 908 1 12992 581 580 25 91 245 2...      2\n","2  [ 10 187 589 1123 6 267 29 54 2 3004 44 2459 1...      2\n","3 카테고리\n","104308 학습용 뉴스 기사\n","26078 테스트용 뉴스 기사\n"]}],"source":["import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Embedding, GlobalAveragePooling1D, Dense, Dropout, LayerNormalization\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import SparseCategoricalCrossentropy\n","from tensorflow.keras.metrics import SparseCategoricalAccuracy\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing import sequence\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Embedding, LSTM, Conv1D, MaxPooling1D\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from attention import Attention\n","\n","class MultiHeadSelfAttention(tf.keras.layers.Layer):\n","    def __init__(self, embed_dim, num_heads=8):\n","        super(MultiHeadSelfAttention, self).__init__()\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads\n","        self.projection_dim = embed_dim // num_heads\n","        self.query_dense = tf.keras.layers.Dense(embed_dim)\n","        self.key_dense = tf.keras.layers.Dense(embed_dim)\n","        self.value_dense = tf.keras.layers.Dense(embed_dim)\n","        self.combine_heads = tf.keras.layers.Dense(embed_dim)\n","\n","    def attention(self, query, key, value):\n","        score = tf.matmul(query, key, transpose_b=True)\n","        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n","        scaled_score = score / tf.math.sqrt(dim_key)\n","        weights = tf.nn.softmax(scaled_score, axis=-1)\n","        output = tf.matmul(weights, value)\n","        return output\n","\n","    def separate_heads(self, x, batch_size):\n","        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","    def call(self, inputs):\n","        batch_size = tf.shape(inputs)[0]\n","        query = self.query_dense(inputs)\n","        key = self.key_dense(inputs)\n","        value = self.value_dense(inputs)\n","        query = self.separate_heads(query, batch_size)\n","        key = self.separate_heads(key, batch_size)\n","        value = self.separate_heads(value, batch_size)\n","        attention = self.attention(query, key, value)\n","        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n","        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n","        output = self.combine_heads(concat_attention)\n","        return output\n","\n","def transformer_encoder(inputs, embed_dim, num_heads, ff_dim, rate=0.1):\n","    attn = MultiHeadSelfAttention(embed_dim, num_heads)(inputs)\n","    attn = Dropout(rate)(attn)\n","    out = LayerNormalization(epsilon=1e-6)(inputs + attn)\n","    ffn = tf.keras.Sequential(\n","        [Dense(ff_dim, activation=\"relu\"), Dense(embed_dim),]\n","    )\n","    ffn_output = ffn(out)\n","    ffn_output = Dropout(rate)(ffn_output)\n","    out = LayerNormalization(epsilon=1e-6)(out + ffn_output)\n","    return out\n","\n","# Define Transformer model\n","def build_transformer_model(max_seq_length, vocab_size, embed_dim, num_heads, ff_dim, num_classes):\n","    inputs = Input(shape=(max_seq_length,))\n","    embedding_layer = Embedding(input_dim=vocab_size, output_dim=embed_dim)(inputs)\n","    transformer_block = transformer_encoder(embedding_layer, embed_dim, num_heads, ff_dim)\n","    pool = GlobalAveragePooling1D()(transformer_block)\n","    outputs = Dense(num_classes, activation='softmax')(pool)\n","    model = Model(inputs=inputs, outputs=outputs)\n","    return model\n","\n","# Define hyperparameters\n","max_seq_length = 50\n","vocab_size = 20000\n","embed_dim = 50\n","num_heads = 5\n","ff_dim = 128\n","num_classes = 3\n","learning_rate = 1e-4\n","\n","# Build and compile the model\n","transformer_model = build_transformer_model(max_seq_length, vocab_size, embed_dim, num_heads, ff_dim, num_classes)\n","optimizer = Adam(learning_rate=learning_rate)\n","loss_fn = SparseCategoricalCrossentropy()\n","accuracy_metric = SparseCategoricalAccuracy()\n","\n","transformer_model.compile(optimizer=optimizer, loss=loss_fn, metrics=[accuracy_metric])\n","# Print model summary\n","#transformer_model.summary()\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","# 분석대상 종목 코드\n","stock_code = '328380'\n","# data load\n","bun_data_df = load_data_bun(stock_code)\n","# 실행 확인을 위한 화면 출력\n","print(bun_data_df.head(3))\n","# data preparation: 데이터X,레이블y 분리\n","X = bun_data_df['text']\n","y = bun_data_df['label']\n","\n","# 문자열 형식인 데이터X를 리스트로 변환\n","X= X.apply(lambda x: [int(i) for i in x.strip('[]').split()])\n","# data set separation: 학습셋, 테스트셋 분리\n","X_train, X_test,y_train, y_test = train_test_split(X,y,test_size=0.2, shuffle=True)\n","# data print, data 확인\n","# 레이블 카테고리 갯수 계산\n","category = np.max(y_train) + 1\n","# 실행 확인을 위한 화면 출력\n","print(category,'카테고리')\n","print(len(X_train),'학습용 뉴스 기사')\n","print(len(X_test), '테스트용 뉴스 기사')\n","#print(X_train.at[0])\n","# pad_sequence 단어 수 맞춰주기\n","# 한 기사당 단어 갯수 제한\n","sequence_length = 50\n","X_train = sequence.pad_sequences(X_train, maxlen= sequence_length)\n","X_test = sequence.pad_sequences(X_test,maxlen=sequence_length)\n","\n","# ** 레이블이 1/0 아닌 경우: 원핫 인코딩 처리\n","# y_train = to_categorical(y_train)\n","# y_test = to_categorical(y_test)\n","\n","# 조기중단 설정, early stopping callback\n","early_stopping_callback = EarlyStopping(\n","    monitor= 'val_loss'                   # 모니터 항목 : val_loss\n","  , patience = 10                          # patiencd    : 5 (epochs)\n",")\n","\n","\n","import tensorflow as tf\n","\n","# 사용자 지정 레이어 등록\n","tf.keras.utils.get_custom_objects()['MultiHeadSelfAttention'] = MultiHeadSelfAttention"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44530,"status":"ok","timestamp":1709025528955,"user":{"displayName":"hyun","userId":"14298231370728061195"},"user_tz":-540},"id":"gXUXi33KOSlN","outputId":"71304f72-1c87-4adc-c68f-0bd5cdd5947a"},"outputs":[{"name":"stdout","output_type":"stream","text":["** Transformer **\n","815/815 [==============================] - 11s 13ms/step - loss: 0.9752 - sparse_categorical_accuracy: 0.5261\n","Model: TFMR_sokbo_328380_b32_e200_c3.hdf5, Test Accuracy: 0.5261\n","\n","815/815 [==============================] - 11s 13ms/step - loss: 0.9839 - sparse_categorical_accuracy: 0.5172\n","Model: TFMR_sokbo_328380_b64_e200_c3.hdf5, Test Accuracy: 0.5172\n","\n","815/815 [==============================] - 11s 13ms/step - loss: 0.9625 - sparse_categorical_accuracy: 0.5357\n","Model: TFMR_sokbo_328380_b128_e200_c3.hdf5, Test Accuracy: 0.5357\n","\n"]}],"source":["print('** Transformer **')\n","#-----------------------------------------------------------------------------------\n","model_name = 'TFMR_sokbo_328380_b32_e200_c3.hdf5'\n","model = load_model(filepath=f'./data/model/{model_name}')#,custom_objects={'Attention': Attention})\n","print(f'Model: {model_name}, Test Accuracy: {model.evaluate(X_test,y_test)[1]:.4f}\\n')\n","#-----------------------------------------------------------------------------------\n","#-----------------------------------------------------------------------------------\n","model_name = 'TFMR_sokbo_328380_b64_e200_c3.hdf5'\n","model = load_model(filepath=f'./data/model/{model_name}',custom_objects={'Attention': Attention})\n","print(f'Model: {model_name}, Test Accuracy: {model.evaluate(X_test,y_test)[1]:.4f}\\n')\n","#-----------------------------------------------------------------------------------\n","#-----------------------------------------------------------------------------------\n","model_name = 'TFMR_sokbo_328380_b128_e200_c3.hdf5'\n","model = load_model(filepath=f'./data/model/{model_name}',custom_objects={'Attention': Attention})\n","print(f'Model: {model_name}, Test Accuracy: {model.evaluate(X_test,y_test)[1]:.4f}\\n')\n","#-----------------------------------------------------------------------------------"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyP7NTEXxXO/W53uiFT3pQrQ","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
