{"cells":[{"cell_type":"markdown","metadata":{"id":"HJhz-jhNTHsE"},"source":["#4. 단어사전만들기\n","- 1. 전체 단어 사전만들기\n","- 2. 2만단어 사전 만들기"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26280,"status":"ok","timestamp":1707304127089,"user":{"displayName":"hyun","userId":"14298231370728061195"},"user_tz":-540},"id":"57puDSh_yxZP","outputId":"6a6abba9-cded-47d1-e9dd-b26abddbb931"},"outputs":[],"source":["# 코랩에서 실행시 pyMySQL을 먼저 설치해야 한다\n","!pip install JPype1\n","# !pip install konlpy\n","!pip install pyMySQL"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GfXKHQd4y1Em"},"outputs":[],"source":["# import packages\n","import os\n","import numpy as np\n","import pandas as pd\n","#import FinanceDataReader as fdr\n","#import konlpy\n","#from konlpy.tag import Okt\n","import pymysql\n","from os import replace\n","import requests\n","from tqdm.notebook import tqdm\n","from datetime import datetime, timedelta\n","from sqlalchemy import create_engine\n","import ast\n","from os import replace\n","import pickle\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.layers import TextVectorization\n","import re\n","import string\n","import itertools\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"15dNKKxrT04Z"},"source":["---\n","1. 전체 단어 사전 만들기\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BSz6SDgwxw6G"},"outputs":[],"source":["# get_oneday_sokbo_from_db() -------------------------------------------\n","# 지정한 날짜의 하루치 뉴스를 DB에서 읽어오기\n","def get_oneday_sokbo_from_db(sdate):\n","  # DB 연결 준비: 뉴스(2줄 속보) 읽어오기\n","  conn = pymysql.connect(host= '호스트주소', port = 포트번호, user=\"아이디\", password=\"비밀번호\", db=\"DB이름\", charset = 'utf8')\n","  # DB에서 뉴스(summary, press, rdate) 추출위한 sql 준비\n","  sql = f\"SELECT x.* FROM DB이름.{sdate}_sokbo x\"\n","  # DB 검색결과를 dataframe에 저장\n","  result_df = pd.read_sql_query(sql, conn)\n","  # 실행확인을 위한 화면 출력\n","  print(sdate)\n","  # DB close\n","  conn.close()\n","  # 결과 반환\n","  return result_df\n","# get_oneday_sokbo_from_db(sdate) :END---------------------------------------\n","\n","# get_sokbo_from_db() -------------------------------------------------------\n","# 주어진 기간의 뉴스 읽어오기\n","def get_sokbo_from_db(startDate,endDate):\n","  # 시작날짜 ddate 복사\n","  ddate = startDate\n","  # 종료 날짜까지 반복하기\n","  result_df = pd.DataFrame()\n","  while pd.to_datetime(ddate).date() <= pd.to_datetime(endDate).date():\n","    # 해당일 하루 뉴스 가져오기\n","    temp_df = get_oneday_sokbo_from_db(ddate)\n","    # 기존 dataframe에 추가 하기\n","    result_df = pd.concat([result_df,temp_df],axis=0,ignore_index=True)\n","    # 자동 날짜 증가 연산: 1일씩 증가\n","    dt_date = pd.to_datetime(ddate).date() + timedelta(days=1)\n","    # 다음 날짜 문자열 추출\n","    ddate= dt_date.strftime(\"%Y%m%d\")\n","  return result_df\n","# -- get_sokbo_from_db() :END------------------------------------------------"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8O44xXzS0jOO"},"outputs":[],"source":["############################################################################\n","# 전체 단어사전 만들기: TextVectorization 커스터마이즈\n","############################################################################\n","# TextVectorization 을 customize해보기\n","# 표준화, 토큰화 함수를 정의해서 지정하기\n","# 리스트 flatten을 위한 itertools 패키지 import\n","\n","# -- custom_standardization_fn() --------------------------------------------\n","# custom_표준화 함수\n","def custom_standardization_fn(string_tensor):\n","    lowercase_string = string_tensor # 영문의 경우 여기서 모두 소문자 처리, 한글은 필요없음\n","    return tf.strings.regex_replace(\n","        # 문장부호 처리: %와 .는 살리고 나머지는 제외\n","        lowercase_string, f\"[{re.escape(string.punctuation.replace('%','').replace('.',''))}]\", \"\") \n","# -- custom_standardization_fn() :END----------------------------------------\n","\n","# -- custom_split_fn()-------------------------------------------------------\n","# custom_토큰화 함수\n","def custom_split_fn(string_tensor):\n","    return tf.strings.split(string_tensor)  # 한글의 경우 형태소 분석 처리 해야 한다\n","# -- custom_split_fn() :END--------------------------------------------------\n"," \n","\n","# custom 표준화, 토큰화 함수 지정해서 TextVectoriztion 생성하기 ---------------\n","text_vectorization = TextVectorization(\n","    # 정수 인덱스로 출력\n","    output_mode=\"int\",\n","    # 표준화 함수를 custom_표준화 함수로 지정\n","    standardize=custom_standardization_fn,\n","    # 토큰화 함수를 custom_토큰화 함수로 지정\n","    split=custom_split_fn,\n",")\n","# custom 표준화, 토큰화 함수 지정해서 TextVectoriztion 생성하기 END------------"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AY5tVYtoS95P"},"outputs":[],"source":["# --네이버증권실시간속보 2줄뉴스 text로 전체 단어사전만들기------------------------------\n","# db로 부터 데이터를 읽어오기\n","df_test = get_sokbo_from_db('20230101','20240202')\n","# 사전만들기 위한 토큰화된 text를 넣은 dataset 리스트 생성\n","dataset = []\n","# 전체 data 반복\n","for i in range (len(df_test)):\n","  # 토큰화된 text 를 하나의 리스트로 만든다\n","  dataset.append(eval(df_test['tokenized_summary'][i]))\n","#  중첩된 리스트를 flatten 시킨다\n","dataset = list(itertools.chain(*dataset))\n","\n","# ============================================\n","# .adapt()로 단어 사전 만들기\n","text_vectorization.adapt(dataset)\n","# ============================================\n","\n","# 어휘사전 확인 .get_vocabulary() 함수\n","text_vectorization.get_vocabulary()\n","\n","# -- 네이버증권실시간속보 2줄뉴스 text로 사전만들기 :END ----------------------"]},{"cell_type":"markdown","metadata":{"id":"2ob_RVctUKMB"},"source":["---\n","- 전체 단어사전 pickle로 저장\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mg14QMYNPaeY"},"outputs":[],"source":["# =======================================================\n","# TextVectorizer 저장, 사전 vocabulary 저장하기\n","# --- 저장 : start ---\n","# Vector for word \"this\"\n","print (text_vectorization(\"LG\"))\n","\n","# Pickle the config and weights\n","pickle.dump({'config': text_vectorization.get_config(),\n","             'weights': text_vectorization.get_weights()}\n","            , open(\"tv_layer.pkl\", \"wb\"))\n","\n","# Later you can unpickle and use\n","# `config` to create object and\n","# `weights` to load the trained weights.\n","# --- 저장 : end ---\n","# ========================================================="]},{"cell_type":"markdown","metadata":{"id":"Fh6e9OxBUXuo"},"source":["---\n","- pickle로 저장된 전체 단어사전 load 하기\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NiLYKfKpFK43"},"outputs":[],"source":["# =========================================================\n","# 저장된 TextVectorizer 사전 vocabulary load 하기\n","# --- 불러오기 load : start ---\n","from_disk = pickle.load(open(\"tv_layer.pkl\", \"rb\"))\n","\n","new_vectorizer = TextVectorization(\n","    # 정수 인덱스로 출력\n","    output_mode=\"int\",\n","    # 표준화 함수를 custom_표준화 함수로 지정\n","    standardize=custom_standardization_fn,\n","    # 토큰화 함수를 custom_토큰화 함수로 지정\n","    split=custom_split_fn,\n","  )\n","new_vectorizer.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\n","new_vectorizer.set_weights(from_disk['weights'])\n","\n","# Lets see the Vector for word \"this\"\n","print (new_vectorizer(\"LG\"))\n","# --- 불러오기 load : end ---\n","# ============================================================="]},{"cell_type":"markdown","metadata":{"id":"TOeRJyMWUs1z"},"source":["---\n","2. 2만 단어 사전 만들기\n","- max_tokens 지정: max_tokens=20000\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mM0DWu1YU9aY"},"outputs":[],"source":["##########################################################\n","# 2만단어 단어사전 만들기: TextVectorization 커스터마이즈\n","##########################################################\n","from tensorflow.keras.layers import TextVectorization\n","# TextVectorization 을 customize해보기\n","# 표준화, 토큰화 함수를 정의해서 지정하기\n","# -- custom_standardization_fn() --------------------------------------------\n","# custom_표준화 함수\n","def custom_standardization_fn(string_tensor):\n","    lowercase_string = string_tensor # 영문의 경우 여기서 모두 소문자 처리, 한글은 필요없음\n","    return tf.strings.regex_replace(\n","        # 문장부호 처리: %와 .는 살리고 나머지는 제외\n","        lowercase_string, f\"[{re.escape(string.punctuation.replace('%','').replace('.',''))}]\", \"\") \n","# -- custom_standardization_fn() :END----------------------------------------\n","\n","# -- custom_split_fn()-------------------------------------------------------\n","# custom_토큰화 함수\n","def custom_split_fn(string_tensor):\n","    return tf.strings.split(string_tensor)  # 한글의 경우 형태소 분석 처리 해야 한다\n","# -- custom_split_fn() :END--------------------------------------------------\n","\n","# custom 표준화, 토큰화 함수 지정해서 TextVectoriztion 생성하기 ---------------\n","text_vectorization = TextVectorization(\n","    #--- max_tokens 지정--\n","    max_tokens=20000\n","    #---------------------\n","    # 정수 인덱스로 출력\n","  ,  output_mode=\"int\"\n","    # 표준화 함수를 custom_표준화 함수로 지정\n","  ,  standardize=custom_standardization_fn\n","    # 토큰화 함수를 custom_토큰화 함수로 지정\n","  ,  split=custom_split_fn\n",")\n","# custom 표준화, 토큰화 함수 지정해서 TextVectoriztion 생성하기 :END------------"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0vezhNSiU9YB"},"outputs":[],"source":["# --네이버증권실시간속보 2줄뉴스 text로 2만단어사전만들기-------------------------\n","# db로 부터 데이터를 읽어오기\n","df_test = get_sokbo_from_db('20230101','20240202')\n","# 사전만들기 위한 토큰화된 text를 넣은 dataset 리스트 생성\n","dataset = []\n","# 전체 data 반복\n","for i in range (len(df_test)):\n","  # 토큰화된 text 를 하나의 리스트로 만든다\n","  dataset.append(eval(df_test['tokenized_summary'][i]))\n","#  중첩된 리스트를 flatten 시킨다\n","dataset = list(itertools.chain(*dataset))\n","\n","# ============================================\n","# .adapt()로 단어 사전 만들기\n","text_vectorization.adapt(dataset)\n","# ============================================\n","\n","# 어휘사전 확인 .get_vocabulary() 함수\n","len(text_vectorization.get_vocabulary())\n","# --네이버증권실시간속보 2줄뉴스 text로 2만단어사전만들기 :END--------------------"]},{"cell_type":"markdown","metadata":{"id":"HphkplkzVX1q"},"source":["---\n","- 2만 단어 사전 pickle로 저장\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lv5BXVcPU9Me"},"outputs":[],"source":["# =======================================================\n","# TextVectorizer 저장, 2만단어 사전 vocabulary 저장하기\n","# --- 저장 : start ---\n","# Vector for word \"this\"\n","print (text_vectorization(\"LG\"))\n","\n","# Pickle the config and weights\n","pickle.dump({'config': text_vectorization.get_config(),\n","             'weights': text_vectorization.get_weights()}\n","            , open(\"tv_layer_20000.pkl\", \"wb\"))\n","\n","# Later you can unpickle and use\n","# `config` to create object and\n","# `weights` to load the trained weights.\n","# --- 저장 : end ---\n","# ========================================================="]},{"cell_type":"markdown","metadata":{"id":"0PiX6FwNVm9f"},"source":["---\n","* pickle로 저장된 2만 단어 사전 load 하기\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b9eDL2tlVhrU"},"outputs":[],"source":["# =========================================================\n","# 저장된 TextVectorizer 2만단어사전 vocabulary load 하기\n","# --- 불러오기 load : start ---\n","from_disk = pickle.load(open(\"tv_layer_20000.pkl\", \"rb\"))\n","\n","new_vectorizer = TextVectorization(\n","    # 정수 인덱스로 출력\n","    output_mode=\"int\",\n","    # 표준화 함수를 custom_표준화 함수로 지정\n","    standardize=custom_standardization_fn,\n","    # 토큰화 함수를 custom_토큰화 함수로 지정\n","    split=custom_split_fn,\n","  )\n","new_vectorizer.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\n","new_vectorizer.set_weights(from_disk['weights'])\n","\n","# Lets see the Vector for word \"this\"\n","print (new_vectorizer(\"LG\"))\n","# --- 불러오기 load : end ---\n","# ============================================================="]}],"metadata":{"colab":{"authorship_tag":"ABX9TyN0f3Rw/HSnVa/BxwH19yGa","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
