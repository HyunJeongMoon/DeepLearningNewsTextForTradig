{"cells":[{"cell_type":"markdown","metadata":{"id":"e0-VsHTHsxqU"},"source":["1. 네이버증권 실시간 속보 뉴스 가져와서 DB에 저장\n","- 2줄 뉴스, 신문사, 날짜를 읽어온다\n","- DB: news.db, 테이블명: 날짜 (ex.20240127)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35776,"status":"ok","timestamp":1708319327439,"user":{"displayName":"hyun","userId":"14298231370728061195"},"user_tz":-540},"id":"5hhtMHDtsYj8","outputId":"664c1ae2-13a2-4de9-9f56-bda3f074df3f"},"outputs":[],"source":["# 코랩에서 실행시 pyMySQL을 먼저 설치해야 한다\n","!pip install JPype1\n","!pip install konlpy\n","!pip install pyMySQL\n","!pip install -U finance-datareader"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":265,"status":"ok","timestamp":1708319380748,"user":{"displayName":"hyun","userId":"14298231370728061195"},"user_tz":-540},"id":"r9laWVl5s3Gm"},"outputs":[],"source":["# import packages\n","import os\n","import numpy as np\n","import pandas as pd\n","import FinanceDataReader as fdr\n","import konlpy\n","from konlpy.tag import Okt\n","import pymysql\n","from os import replace\n","import requests\n","from tqdm.notebook import tqdm\n","from datetime import datetime\n","from sqlalchemy import create_engine\n","import ast\n","from os import replace\n","from bs4 import BeautifulSoup"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":530399,"status":"ok","timestamp":1708320006182,"user":{"displayName":"hyun","userId":"14298231370728061195"},"user_tz":-540},"id":"OjY0taHMs8AC","outputId":"f7f574f9-0d9d-432d-e291-a489257c2b10"},"outputs":[],"source":["# ########################################################################\n","# 네이버증권 실시간 속보 뉴스 크롤링 가져오기\n","# 기간 입력 예: 시작날짜(20231101), 종료날짜(20240119)\n","# 출력: GCP Maria DB 로 저장 , DB 이름: news.db, 날짜별 테이블로 저장\n","# ########################################################################\n","\n","# -- ex_news_onePage() ---------------------------------------------------\n","# 한 페이지에 있는 뉴스 가져오기: 페이지당 20개 뉴스 있음\n","# 가져오는 항목: 2줄 요약 summary, 신문사 press, 배포날짜 rdate\n","# BeautifulSoup 객체를 파라미터로 받기\n","def ex_news_onePage(soup):\n","  # 복사본 만들기\n","  soup_1 = soup\n","  # <dd class='articleSummary'> 태그 목록 만들기\n","  ddsum_list = soup_1.find_all('dd', attrs={\"class\": \"articleSummary\"})\n","  # 현재 페이지 전체 뉴스 저장을 위한 news_df 생성\n","  news_df = pd.DataFrame()\n","  # 제일 마지막 페이지에는 <dd class='articleSummary'> 태그 내용이 없음, 뉴스 있는 동안 반복\n","  if len(ddsum_list)>0:\n","    # <dd> 태그 목록에서 하나씩 꺼내기\n","    for ddsum in ddsum_list:\n","      # <dd> 와 </dd> 사이의 텍스트를 추출, 다른 태그 있을시 '^'구분해서 추출, 앞뒤 공백제거\n","      data_string = ddsum.getText('^').strip()\n","      # 추출한 문자열을 '^' 경계로 분리\n","      string_list = data_string.split('^')\n","      # 2줄 요약 news_summary 문자열 추출\n","      news_summary  = string_list[0].strip()\n","      # 신문사 press_name 문자열 추출\n","      press_name    = string_list[1].strip()\n","      # 배포날짜 release_date 문자열 추출\n","      release_date  = string_list[5].strip()\n","      # 추출한 위 3 항목을 DataFrame으로 저장\n","      temp_df = pd.DataFrame(data = [[news_summary,press_name,release_date]], columns=['summary','press','rdate'])\n","      # 현재 페이지 전체 뉴스를 위한 news_df에 추가\n","      news_df = pd.concat([news_df, temp_df], ignore_index=True)\n","  # 현재 페이지 전체 뉴스를 위한 news_df 반환\n","  return news_df\n","# -- ex_news_onePage() :END -----------------------------------------------------\n","\n","# -- ex_news_oneDay() -----------------------------------------------------------\n","# 해당일 뉴스 전체를 가져오기: 페이지 1 부터 끝까지, 1일 10~50여개 페이지(페이지당 20개 뉴스 있음)\n","# 날짜 date 파라미터로 받아서, 네이버증권실시간속보 웹 페이지에 접속\n","# url: f\"https://finance.naver.com/news/news_list.naver?mode=LSS2D&section_id=101&section_id2=258&date={날짜}&page={페이지번호}\"\n","# 뉴스 가져올 날짜 date 파라미터로 받기\n","def ex_news_oneDay(date):\n","  # 파라미터 날짜 date 복사\n","  t_target = date\n","  # 페이지번호 초기화\n","  page_num = 0\n","  # 해당일 전체 뉴스 저장을 위한 데이터프레임 df_one_day_news 생성\n","  df_one_day_news = pd.DataFrame()\n","  # while True: 무한 반복문 사용, 날짜별로 뉴스 페이지 갯수가 다르므로 뉴스가 없을 때 반복 종료\n","  while True:\n","    # 페이지번호 1씩 증가\n","    page_num +=1\n","    # 크롤링 할 웹주소 URL, f-string 으로 날짜t_target, 페이지번호page_num 대입\n","    url_1= f\"https://finance.naver.com/news/news_list.naver?mode=LSS2D&section_id=101&section_id2=258&date={t_target}&page={page_num}\"\n","    # 웹 request 위한 html 형식 설정\n","    html = requests.get(url_1, headers={\"User-Agent\": \"Mozilla/5.0\"\\\n","    \"(Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\\\n","    \"Chrome/110.0.0.0 Safari/537.36\"})\n","    # BeautifulSoup 으로 해당 웹페이지 읽어오기\n","    soup = BeautifulSoup(html.text, \"lxml\")\n","    # 해당 페이지의 뉴스 추출 함수 호출, 한 페이지 뉴스 저장 df_onepagenews\n","    df_onepagenews= ex_news_onePage(soup)\n","    # 해당 페이지에 뉴스가 있으면 당일 전체 뉴스 저장 df_one_day_news 에 추가\n","    if len(df_onepagenews)>0:\n","      df_one_day_news = pd.concat([df_one_day_news,df_onepagenews],ignore_index=True)\n","    # 해당 페이지에 뉴스가 더이상 없으면 반복 종료 break\n","    else:\n","      break\n","  # 해당일 전체 뉴스 df_one_day_news 반환\n","  return df_one_day_news\n","# -- ex_news_oneDay() :END--------------------------------------------------------\n","\n","# -- newsDBsave() ----------------------------------------------------------------\n","# 추출한 일일 뉴스 데이터를 클라우드 MariaDB에 저장\n","# DB명: news.db, 테이블명: 날짜ddate\n","# 날짜ddate와 해당일 전체 뉴스 저장한 데이터프레임 news_all 을 파라미터로 받기\n","def newsDBsave(ddate, news_all):\n","  # 테이블 생성 및 연결\n","  # 구글클라우드 마리아DB연결 예)'mysql+pymysql://root:mypassword@localhost:1234/testdb'\n","  db_connection_path = 'mysql+pymysql://아이디:패스워드@호스트주소:포트번호/DB이름'\n","  db_connection = create_engine(db_connection_path)\n","  # DB 커넥션 생성\n","  conn = db_connection.connect()\n","  # 데이터프레임을 DB로 저장, 날짜 ddate를 테이블 명으로, 이미 있으면 덮어쓰기(replace)\n","  news_all.to_sql(ddate,conn,if_exists='replace')\n","  # DB 커밋\n","  conn.commit()\n","  # DB close\n","  conn.close()\n","# -- newsDBsave() :END-------------------------------------------------------------\n","\n","# -- getNewsFromNaver() -----------------------------------------------------------\n","# 지정된 기간의 네이버증권실시간속보 뉴스 가져오기\n","# 날짜는 'YYYYMMDD'형식 문자열\n","# 시작날짜 startDate, 종료날짜 endDate 를 파라미터로 받기\n","def getNewsFromNaver(startDate,endDate):\n","  # 시작날짜 ddate 복사\n","  ddate = startDate\n","  # 종료 날짜까지 반복하기\n","  while pd.to_datetime(ddate).date() <= pd.to_datetime(endDate).date():\n","    # 해당일 전체 뉴스 가져오기\n","    news_all = ex_news_oneDay(ddate)\n","    # 해당일 전체 뉴스 DB에 저장하기\n","    newsDBsave(ddate, news_all)\n","    # 실행중 확인을 위한 날짜 화면 출력\n","    print(ddate)\n","    # 자동 날짜 증가 연산: 1일씩 증가\n","    dt_date = pd.to_datetime(ddate) + pd.DateOffset(days=1)\n","    # 다음 날짜 문자열 추출\n","    ddate= dt_date.strftime(\"%Y%m%d\")\n","# -- getNewsFromNaver() :END--------------------------------------------------------\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# --메인 함수 호출 ------------------------------------------------------------------\n","# 웹 크롤링으로 뉴스 가져올 기간을 파라미터로 넘기기\n","# 예) 시작날짜 '20240117', 종료날짜 '20240119'인 경우\n","# getNewsFromNaver('20240117','20240119') 로 함수 호출\n","\n","getNewsFromNaver('20240203','20240216')\n","\n","# 오늘 현재 시점 now 하루 뉴스 가져오기 하려면 아래 함수 호출\n","#datetime.now().strftime('%Y%m%d')\n","#getNewsFromNaver(datetime.now().strftime('%Y%m%d'), datetime.now().strftime('%Y%m%d'))\n","# --메인 함수 호출 :END--------------------------------------------------------------"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO1wcJxzbrr8vycQJWDWVSL","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
