{"cells":[{"cell_type":"markdown","metadata":{"id":"i1ofXr0W9Vxn"},"source":["#* 오늘 속보 처리 함수\n","- 1. 설치 및 konlpy update\n","- 2. 네이버증권 실시간뉴스 속보 읽어오기\n","- 3. konlpy로 토큰화하기\n","- 4. 2만단어 사전으로 TextVectorization 이용해서 인코딩\n","- 5. 2만단어 사전으로 TextVectorization 이용해서 디코딩"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33068,"status":"ok","timestamp":1707743381678,"user":{"displayName":"hyun","userId":"14298231370728061195"},"user_tz":-540},"id":"S0MeyGPP-E-y","outputId":"d75565b5-2d7a-4970-80ab-aabe054f06ee"},"outputs":[],"source":["# 코랩에서 실행시 pyMySQL을 먼저 설치해야 한다\n","# !pip install JPype1\n","# !pip install konlpy\n","# !pip install pyMySQL\n","# !pip install -U finance-datareader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iAF0rFGu-kfy"},"outputs":[],"source":["# import packages\n","# import packages\n","import os\n","import numpy as np\n","import pandas as pd\n","import FinanceDataReader as fdr\n","import konlpy\n","from konlpy.tag import Okt\n","import pymysql\n","from os import replace\n","import requests\n","from bs4 import BeautifulSoup\n","from tqdm.notebook import tqdm\n","from datetime import datetime\n","from sqlalchemy import create_engine\n","import ast\n","from os import replace\n","import pickle\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.layers import TextVectorization\n","import re\n","import string\n","import itertools\n","import pytz\n","from tensorflow.keras.layers import Dense, Dropout, Activation \\\n","                                  , Embedding, LSTM, Conv1D, MaxPooling1D\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.preprocessing import sequence\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from tensorflow.keras.models import Model, Sequential,load_model\n","from attention import Attention\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# import END------------------------------"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4038,"status":"ok","timestamp":1707743521552,"user":{"displayName":"hyun","userId":"14298231370728061195"},"user_tz":-540},"id":"O8KSwXNRLskL","outputId":"5b64ee3f-5ca9-42cb-af4a-b306fd5e3d55"},"outputs":[],"source":["# --konlpy 커스터마이징  ----------------------------------------------\n","# --------------------------------------------------------------------\n","# Konlpy에 한국거래소 상장종목 전체 종목명과 약명을 새로운 단어로 추가하기\n","# --------------------------------------------------------------------\n","# konlpy 디렉토리로 이동\n","os.chdir('konlpy/설치된/디렉토리/java')\n","# 현재 디렉토리 확인\n","os.getcwd()\n","# 압축 풀 임시 디렉토리 /temp 만들기\n","os.makedirs('./temp')\n","# 임시디렉토리로 이동\n","os.chdir('./temp')\n","# 압축 풀기\n","!jar xvf ../open-korean-text-2.1.0.jar\n","\n","# FinanceDataReader로 한국거래소 상장종목 전체 종목을 가져오기\n","df_krx = fdr.StockListing('KRX')\n","# 종목명만 추출하기\n","name_list = df_krx['Name']\n","# 새로운 단어목록 생성을 위한 data 변수 선언\n","data = ''\n","# 종목명을 한줄씩(\\n)씩 넣어서 목록 만들기\n","for one_name in name_list:\n","  data += one_name+'\\n'\n","# 종목명 약명 새로운 단어로 넣기\n","# DB 연결 준비: 종목명 약어 읽어오기\n","conn = pymysql.connect(host= '호스트주소', port = 포트번호, user=\"아이디\", password=\"패스워드\", db=\"DB이름\", charset = 'utf8')\n","# DB에서 종목병 약어 추출위한 sql 준비\n","sql = f\"SELECT x.kor_name,x.kor_abb FROM DB이름.테이블이름 x\"\n","# DB 검색결과를 dataframe에 저장\n","name_abb_df = pd.read_sql_query(sql, conn)\n","# 실행확인을 위한 화면 출력\n","print(name_abb_df.head(3))\n","# DB close\n","conn.close()\n","# 종목명 추가하기\n","name_list = name_abb_df['kor_name']\n","# 종목명을 한줄씩(\\n)씩 넣어서 목록 만들기\n","for one_name in name_list:\n","  data += one_name+'\\n'\n","# 종목명 약명 추가하기\n","name_list = name_abb_df['kor_abb']\n","# 종목명 약명을 한줄씩(\\n)씩 넣어서 목록 만들기\n","for one_name in name_list:\n","  data += one_name+'\\n'\n","# konlpy의 company_names.txt 사전으로 저장\n","with open(\"konlpy/설치된/디렉토리/java/temp/org/openkoreantext/processor/util/noun/company_names.txt\", 'w') as f:\n","    f.write(data)\n","# 임시 작업 디렉토리로 이동 확인\n","os.chdir('konlpy/설치된/디렉토리/java/temp')\n","# 다시 압축\n","!jar cvf ../open-korean-text-2.1.0.jar *\n","# 코랩 실행시 Restart session 실행\n","\n","# --konlpy 커스터마이징 END --------------------------------------------"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WSAbQ85XL1P-"},"outputs":[],"source":["# !! 코랩 실행시 import 다시하기 !! --------------------------------\n","# import packages\n","import os\n","import numpy as np\n","import pandas as pd\n","import FinanceDataReader as fdr\n","import konlpy\n","from konlpy.tag import Okt\n","import pymysql\n","from os import replace\n","import requests\n","from bs4 import BeautifulSoup\n","from tqdm.notebook import tqdm\n","from datetime import datetime\n","from sqlalchemy import create_engine\n","import ast\n","from os import replace\n","import pickle\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.layers import TextVectorization\n","import re\n","import string\n","import itertools\n","import pytz\n","from tensorflow.keras.layers import Dense, Dropout, Activation \\\n","                                  , Embedding, LSTM, Conv1D, MaxPooling1D\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.preprocessing import sequence\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from tensorflow.keras.models import Model, Sequential,load_model\n","from attention import Attention\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# import END------------------------------"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# --konlpy 커스터마이징 확인 ----------------------------------------------\n","# konlpy 커스터마이징 확인용 코드\n","# 종목명 토크나이즈 확인할 것: 삼성전자 -> '삼성','전자'(X), '삼성전자'(O)\n","# 제대로 안되면 konlpy 커스터마이징 확인\n","import konlpy\n","from konlpy.tag import Okt\n","\n","okt = Okt()\n","print(okt.pos(\"삼성전자 005930 \"))\n","# --konlpy 커스터마이징 확인 End ------------------------------------------"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fgVfVTu8-sEz"},"outputs":[],"source":["# ########################################################################\n","# 네이버증권 실시간 속보 뉴스 크롤링 가져오기\n","# 기간 입력 예: 시작날짜(20231101), 종료날짜(20240119)\n","# 출력: GCP Maria DB 로 저장 , DB 이름: news.db, 날짜별 테이블로 저장\n","# ########################################################################\n","\n","# # -- ex_news_onePage() ---------------------------------------------------\n","# 한 페이지에 있는 뉴스 가져오기: 페이지당 20개 뉴스 있음\n","# 가져오는 항목: 2줄 요약 summary, 신문사 press, 배포날짜 rdate\n","# BeautifulSoup 객체를 파라미터로 받기\n","def ex_news_onePage(soup):\n","  # 복사본 만들기\n","  soup_1 = soup\n","  # <dd class='articleSummary'> 태그 목록 만들기\n","  ddsum_list = soup_1.find_all('dd', attrs={\"class\": \"articleSummary\"})\n","  # 현재 페이지 전체 뉴스 저장을 위한 news_df 생성\n","  news_df = pd.DataFrame()\n","  # 제일 마지막 페이지에는 <dd class='articleSummary'> 태그 내용이 없음, 뉴스 있는 동안 반복\n","  if len(ddsum_list)>0:\n","    # <dd> 태그 목록에서 하나씩 꺼내기\n","    for ddsum in ddsum_list:\n","      # <dd> 와 </dd> 사이의 텍스트를 추출, 다른 태그 있을시 '^'구분해서 추출, 앞뒤 공백제거\n","      data_string = ddsum.getText('^').strip()\n","      # 추출한 문자열을 '^' 경계로 분리\n","      string_list = data_string.split('^')\n","      # 2줄 요약 news_summary 문자열 추출\n","      news_summary  = string_list[0].strip()\n","      # 신문사 press_name 문자열 추출\n","      press_name    = string_list[1].strip()\n","      # 배포날짜 release_date 문자열 추출\n","      release_date  = string_list[5].strip()\n","      # 추출한 위 3 항목을 DataFrame으로 저장\n","      temp_df = pd.DataFrame(data = [[news_summary,press_name,release_date]], columns=['summary','press','rdate'])\n","      # 현재 페이지 전체 뉴스를 위한 news_df에 추가\n","      news_df = pd.concat([news_df, temp_df], ignore_index=True)\n","  # 현재 페이지 전체 뉴스를 위한 news_df 반환\n","  return news_df\n","# -- ex_news_onePage() :END -----------------------------------------------------\n","\n","# -- ex_news_oneDay() -----------------------------------------------------------\n","# 해당일 뉴스 전체를 가져오기: 페이지 1 부터 끝까지, 1일 10~50여개 페이지(페이지당 20개 뉴스 있음)\n","# 날짜 date 파라미터로 받아서, 네이버증권실시간속보 웹 페이지에 접속\n","# url: f\"https://finance.naver.com/news/news_list.naver?mode=LSS2D&section_id=101&section_id2=258&date={날짜}&page={페이지번호}\"\n","# 뉴스 가져올 날짜 date 파라미터로 받기\n","def ex_news_oneDay(date):\n","  # 파라미터 날짜 date 복사\n","  t_target = date\n","  # 페이지번호 초기화\n","  page_num = 0\n","  # 해당일 전체 뉴스 저장을 위한 데이터프레임 df_one_day_news 생성\n","  df_one_day_news = pd.DataFrame()\n","  # while True: 무한 반복문 사용, 날짜별로 뉴스 페이지 갯수가 다르므로 뉴스가 없을 때 반복 종료\n","  while True:\n","    # 페이지번호 1씩 증가\n","    page_num +=1\n","    # 크롤링 할 웹주소 URL, f-string 으로 날짜t_target, 페이지번호page_num 대입\n","    url_1= f\"https://finance.naver.com/news/news_list.naver?mode=LSS2D&section_id=101&section_id2=258&date={t_target}&page={page_num}\"\n","    # 웹 request 위한 html 형식 설정\n","    html = requests.get(url_1, headers={\"User-Agent\": \"Mozilla/5.0\"\\\n","    \"(Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\\\n","    \"Chrome/110.0.0.0 Safari/537.36\"})\n","    # BeautifulSoup 으로 해당 웹페이지 읽어오기\n","    soup = BeautifulSoup(html.text, \"lxml\")\n","    # 해당 페이지의 뉴스 추출 함수 호출, 한 페이지 뉴스 저장 df_onepagenews\n","    df_onepagenews= ex_news_onePage(soup)\n","    # 해당 페이지에 뉴스가 있으면 당일 전체 뉴스 저장 df_one_day_news 에 추가\n","    if len(df_onepagenews)>0:\n","      df_one_day_news = pd.concat([df_one_day_news,df_onepagenews],ignore_index=True)\n","    # 해당 페이지에 뉴스가 더이상 없으면 반복 종료 break\n","    else:\n","      break\n","  # 해당일 전체 뉴스 df_one_day_news 반환\n","  return df_one_day_news\n","# -- ex_news_oneDay() :END--------------------------------------------------------\n","\n","# -- newsDBsave() ----------------------------------------------------------------\n","# 추출한 일일 뉴스 데이터를 클라우드 MariaDB에 저장\n","# 날짜ddate와 해당일 전체 뉴스 저장한 데이터프레임 news_all 을 파라미터로 받기\n","def newsDBsave(ddate, news_all):\n","  # 테이블 생성 및 연결\n","  # 구글클라우드 마리아DB연결 예)'mysql+pymysql://root:mypassword@localhost:1234/testdb'\n","  db_connection_path = 'mysql+pymysql://아이디:패스워드@호스트주소:포트번호/DB이름'\n","  db_connection = create_engine(db_connection_path)\n","  # DB 커넥션 생성\n","  conn = db_connection.connect()\n","  # 데이터프레임을 DB로 저장, 날짜 ddate를 테이블 명으로, 이미 있으면 덮어쓰기(replace)\n","  news_all.to_sql(ddate,conn,if_exists='replace')\n","  # DB 커밋\n","  conn.commit()\n","  # DB close\n","  conn.close()\n","# -- newsDBsave() :END-------------------------------------------------------------\n","\n","# -- today_sokbo_DBsave() ---------------------------------------------------------\n","# 당일 속보를 DB에 저장\n","def today_sokbo_DBsave(ddate, news_all):\n","  # 테이블 생성 및 연결\n","  # 구글클라우드 마리아DB연결 예)'mysql+pymysql://root:mypassword@localhost:1234/testdb'\n","  db_connection_path = 'mysql+pymysql://아이디:패스워드@호스트주소:포트번호/DB이름'\n","  db_connection = create_engine(db_connection_path)\n","  # DB 커넥션 생성\n","  conn = db_connection.connect()\n","  # 데이터프레임을 DB로 저장, 날짜 ddate를 테이블 명으로, 이미 있으면 덮어쓰기(replace)\n","  news_all.to_sql('today_sokbo',conn,if_exists='replace')\n","  # DB 커밋\n","  conn.commit()\n","  # DB close\n","  conn.close()\n","# -- today_sokbo_DBsave() :END--------------------------------------------------------\n","# -- get_today_news_sokbo() ----------------------------------------------------------\n","def get_today_news_sokbo():\n","  # 오늘 현재 시점 날자, GCP locale이 한국이 아닐 수도 있으니 timezone 맞춘다\n","  ddate = datetime.now(pytz.timezone('Asia/Seoul')).strftime('%Y%m%d')\n","  # 해당일 전체 뉴스 가져오기\n","  news_all = ex_news_oneDay(ddate)\n","  # 해당일 전체 뉴스 원본(news) DB에 저장하기\n","  newsDBsave(ddate, news_all)\n","  # 해당일 뉴스 속보(today_sokbo) DB에 저장하기\n","  today_sokbo_DBsave(ddate,news_all)\n","  # 실행중 확인을 위한 날짜 화면 출력\n","  print('today:',ddate)\n","# -- get_today_news_sokbo() :END--------------------------------------------------------\n"]},{"cell_type":"markdown","metadata":{"id":"buD6SOS9DLw2"},"source":["2. konlpy로 토큰화하기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dPXLoV-PE-d2"},"outputs":[],"source":["## -- tokenize_today_news_summary() ----------------------------------------------------\n","# 오늘 속보 뉴스 summary 를 tokenize 하기\n","# 특정일 날짜: YYYYMMDD ex) 20240105 (문자열)\n","def tokenize_today_news_summary():\n","  # 오늘 현재 시점 날자, GCP locale이 한국이 아닐 수도 있으니 timezone 맞춘다\n","  sdate = datetime.now(pytz.timezone('Asia/Seoul')).strftime('%Y%m%d')\n","  # DB 연결 준비: 뉴스(2줄 속보) 읽어오기\n","  conn = pymysql.connect(host= '호스트주소', port = 포트번호, user=\"아이디\", password=\"패스워드\", db=\"DB이름\", charset = 'utf8')\n","  # DB에서 뉴스(summary, press, rdate) 추출위한 sql 준비\n","  sql = f\"SELECT x.summary,x.press, x.rdate FROM DB이름.{sdate} x\"\n","  # DB 검색결과를 dataframe에 저장\n","  result_df = pd.read_sql_query(sql, conn)\n","  # 실행확인을 위한 화면 출력\n","  #print(result_df.head(3))\n","  # DB close\n","  conn.close()\n","\n","  # konlpy의 okt 생성\n","  okt=Okt()\n","\n","  # okt로 tokenize해서 형태소 품사 morph 추출하기 (pos 는 okt.pos, 명사는 okt.nouns)\n","  result_df['morphs_tokenized_summary'] = result_df['summary'].apply(okt.morphs)\n","  # DB 저장을 위해 문자열로 변환\n","  result_df['morphs_tokenized_summary'] = result_df['morphs_tokenized_summary'].astype(str)\n","\n","  # okt로 tokenize해서 형태소 pos 추출하기 (품사 전체는 okt.morphs, 명사는 okt.nouns)\n","  result_df['pos_tokenized_summary'] = result_df['summary'].apply(okt.pos)\n","  \n","  # DB 저장을 위해 문자열로 변환\n","  result_df['pos_tokenized_summary'] = result_df['pos_tokenized_summary'].astype(str)\n","\n","  # okt로 tokenize해서 명사 형태소 nouns 추출하기 (품사 전체는 okt.morphs)\n","  result_df['nouns_tokenized_summary'] = result_df['summary'].apply(okt.nouns)\n","  \n","  # DB 저장을 위해 문자열로 변환\n","  result_df['nouns_tokenized_summary'] = result_df['nouns_tokenized_summary'].astype(str)\n","\n","  # pos tokenized 에서 Modifier,Josa,Suffix,Punctuation,Foreigh 제외, 1 자리 단어 제외\n","  result_df['tokenized_summary'] = \"\"\n","  for i in range(len(result_df)):\n","    # 새로운 문자열 위한 리스트\n","    new_str = []\n","    # 튜플 형태로 꺼내면 (단어,품사) 형태이므로 뒤세 품사를 보고 맞는 단어들만 새로운 문자열로 만든다\n","    for (x,y) in ast.literal_eval(result_df['pos_tokenized_summary'][i]):\n","      # 관형사,조사,접두사,문장부호,한자및 기타기호 제외\n","      if (y!='Modifier')&(y!='Josa')&(y!='Suffix')&(y!='Punctuation')&(y!='Foreign'):\n","        # 한글자 짜리 제외\n","        if(len(str(x))>1):\n","          new_str.append(x)\n","    # 실행 확인을 위한 화면 출력\n","    #print(new_str)\n","    # 토크나이즈된 문자열 데이터프레임에 넣기\n","    result_df['tokenized_summary'][i]=str(new_str)\n","\n","  # DB 저장을 위해서 필요한 컬럼 정리\n","  result_df = result_df[['summary','press','rdate','tokenized_summary','pos_tokenized_summary']]\n","  # 실행 확인을 위한 화면 출력\n","  #print(result_df.head(3))\n","\n","  # DB 저장을 위한 연결 준비\n","  db_connection_path = 'mysql+pymysql://아이디:패스워드@호스트주소:포트번호/DB이름'\n","  db_connection = create_engine(db_connection_path)\n","  # DB 커넥션 생성\n","  conn = db_connection.connect()\n","  # 데이터프레임을 DB로 저장, 날짜 ddate_sokbo를 테이블 명으로, 이미 있으면 덮어쓰기(replace)\n","  result_df.to_sql(sdate+'_sokbo',conn,if_exists='replace')\n","  result_df.to_sql('today_sokbo',conn,if_exists='replace')\n","  # DB 커밋\n","  conn.commit()\n","  # DB close\n","  conn.close()\n","\n","# -- tokenize_today_news_summary() :END---------------------------------------------"]},{"cell_type":"markdown","metadata":{"id":"5A7kCZzgHlSn"},"source":["3. 2만단어로 인코딩하기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pACMIvI3Hpk-"},"outputs":[],"source":["# -- get_oneday_sokbo_from_db()------------------------------------------\n","# DB에서 지정된 날짜 하루치 속보 읽어오기\n","def get_oneday_sokbo_from_db(sdate):\n","  # DB 연결 준비: 뉴스(2줄 속보) 읽어오기\n","  conn = pymysql.connect(host= '호스트주소', port = 포트번호, user=\"아이디\", password=\"비밀번호\", db=\"DB이름\", charset = 'utf8')\n","  # DB에서 뉴스(summary, press, rdate) 추출위한 sql 준비\n","  sql = f\"SELECT x.* FROM DB이름.{sdate}_sokbo x\"\n","  # DB 검색결과를 dataframe에 저장\n","  result_df = pd.read_sql_query(sql, conn)\n","  # 실행확인을 위한 화면 출력\n","  print(sdate)\n","  # DB close\n","  conn.close()\n","  # 결과 반환\n","  return result_df\n","# -- get_oneday_sokbo_from_db() END:-------------------------------------\n","\n","\n","# -- get_sokbo_from_db() -------------------------------------------------\n","# 주어진 기간의 뉴스 읽어오기\n","def get_sokbo_from_db(startDate,endDate):\n","  # 시작날짜 ddate 복사\n","  ddate = startDate\n","  # 종료 날짜까지 반복하기\n","  result_df = pd.DataFrame()\n","  while pd.to_datetime(ddate).date() <= pd.to_datetime(endDate).date():\n","    # 해당일 하루 뉴스 가져오기\n","    temp_df = get_oneday_sokbo_from_db(ddate)\n","    # 기존 dataframe에 추가 하기\n","    result_df = pd.concat([result_df,temp_df],axis=0,ignore_index=True)\n","    # 자동 날짜 증가 연산: 1일씩 증가\n","    dt_date = pd.to_datetime(ddate).date() + timedelta(days=1)\n","    # 다음 날짜 문자열 추출\n","    ddate= dt_date.strftime(\"%Y%m%d\")\n","  return result_df\n","# -- get_sokbo_from_db() :END---------------------------------------------\n","\n","# -- custom_standardization_fn() --------------------------------------------\n","# custom_표준화 함수\n","def custom_standardization_fn(string_tensor):\n","    lowercase_string = string_tensor # 영문의 경우 여기서 모두 소문자 처리, 한글은 필요없음\n","    return tf.strings.regex_replace(\n","        # 문장부호 처리: %와 .는 살리고 나머지는 제외\n","        lowercase_string, f\"[{re.escape(string.punctuation.replace('%','').replace('.',''))}]\", \"\") \n","# -- custom_standardization_fn() :END----------------------------------------\n","\n","# -- custom_split_fn()-------------------------------------------------------\n","# custom_토큰화 함수\n","def custom_split_fn(string_tensor):\n","    return tf.strings.split(string_tensor)  # 한글의 경우 형태소 분석 처리 해야 한다\n","# -- custom_split_fn() :END--------------------------------------------------\n"," \n","# custom 표준화, 토큰화 함수 지정해서 TextVectoriztion 생성하기\n","text_vectorization = TextVectorization(\n","    # max_tokens 지정\n","    max_tokens=20000\n","    # 정수 인덱스로 출력\n","  ,  output_mode=\"int\"\n","    # 표준화 함수를 custom_표준화 함수로 지정\n","  ,  standardize=custom_standardization_fn\n","    # 토큰화 함수를 custom_토큰화 함수로 지정\n","  ,  split=custom_split_fn\n","\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oXmxq3dAIhqf"},"outputs":[],"source":["#  -- encode_save_today_sokbo_20000() --------------------------------------\n","# 2만 단어사전으로 인코딩한 결과 DB에 저장하기\n","def encode_save_today_sokbo_20000():\n","   # 오늘 현재 시점 날자, GCP locale이 한국이 아닐 수도 있으니 timezone 맞춘다\n","  sdate = datetime.now(pytz.timezone('Asia/Seoul')).strftime('%Y%m%d')\n","  \n","  # 1. 2만단어사전 불러오기 load : start --------------\n","  from_disk = pickle.load(open(\"사전경로/tv_layer_20000.pkl\", \"rb\"))\n","\n","  new_vectorizer = TextVectorization(\n","      # max_tokens 지정\n","      max_tokens=20000\n","      # 정수 인덱스로 출력\n","    ,  output_mode=\"int\"\n","      # 표준화 함수를 custom_표준화 함수로 지정\n","    ,  standardize=custom_standardization_fn\n","      # 토큰화 함수를 custom_토큰화 함수로 지정\n","    ,  split=custom_split_fn\n","    )\n","  new_vectorizer.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\n","  new_vectorizer.set_weights(from_disk['weights'])\n","  # --- 2만단어사전 불러오기 load : end ---\n","\n","  # 2. 속보뉴스 DB에서 가져오기\n","  # 구글클라우드 마리아DB연결 예)'mysql+pymysql://root:mypassword@localhost:1234/testdb'\n","  db_connection_path = 'mysql+pymysql://아이디:패스워드@호스트주소:포트번호/DB이름'\n","  db_connection = create_engine(db_connection_path)\n","  # DB 커넥션 생성\n","  conn = db_connection.connect()\n","  # 해당 날짜 하루치 속보 DB에서 읽어오기\n","  df_oneday = get_oneday_sokbo_from_db(sdate)\n","  # 위 생성한 백터라이즈를 하루치 속보에 적용 \n","  df_oneday['20000_encoded_text']= list(map(new_vectorizer,df_oneday['tokenized_summary']))\n","  for i in range(len(df_oneday)):\n","    # 숫자로 인코드된 뉴스를 문자열로 만들어서 DB에 저장\n","    df_oneday.at[i,'20000_encoded_text_code']= str(new_vectorizer(df_oneday.at[i,'tokenized_summary']).numpy())\n","  # 실행중 확인을 위한 날짜 화면 출력\n","  print(sdate)\n","  # 데이터프레임을 DB로 저장, 날짜 ddate를 테이블 명으로, 이미 있으면 덮어쓰기(replace)\n","  df_oneday.to_sql(sdate+'_sokbo',conn,if_exists='replace',index=False)\n","  # 오늘 속보를 위한 테이블로도 저장 \n","  df_oneday.to_sql('today_sokbo',conn,if_exists='replace',index=False)\n","\n","  # DB 커밋\n","  conn.commit()\n","  # DB close\n","  conn.close()\n","\n","#  -- encode_save_today_sokbo_20000() :END ------------------------------\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fn7q4e_pKHcM"},"outputs":[],"source":["# -- decode_today_sokbo_20000()-----------------------------------------\n","# 2만 단어사전으로 디코딩하기\n","def decode_today_sokbo_20000():\n","  # 오늘 현재 시점 날자, GCP locale이 한국이 아닐 수도 있으니 timezone 맞춘다\n","  sdate = datetime.now(pytz.timezone('Asia/Seoul')).strftime('%Y%m%d')\n","  # 1. 2만단어사전 불러오기 load : start ---\n","  from_disk = pickle.load(open(\"tv_layer_20000.pkl\", \"rb\"))\n","\n","  new_vectorizer = TextVectorization(\n","      # max_tokens 지정\n","      max_tokens=20000\n","      # 정수 인덱스로 출력\n","    ,  output_mode=\"int\"\n","      # 표준화 함수를 custom_표준화 함수로 지정\n","    ,  standardize=custom_standardization_fn\n","      # 토큰화 함수를 custom_토큰화 함수로 지정\n","    ,  split=custom_split_fn\n","    )\n","  new_vectorizer.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\n","  new_vectorizer.set_weights(from_disk['weights'])\n","  # --- 2만단어사전 불러오기 load : end ---\n","\n","  # 2. 속보뉴스 DB에서 가져오기\n","  # 구글클라우드 마리아DB연결 예)'mysql+pymysql://root:mypassword@localhost:1234/testdb'\n","  db_connection_path = 'mysql+pymysql://아이디:패스워드@호스트주소:포트번호/DB이름'\n","  db_connection = create_engine(db_connection_path)\n","  # DB 커넥션 생성\n","  conn = db_connection.connect()\n","  # 단어사전 준비\n","  vocabulary = new_vectorizer.get_vocabulary()\n","  # 디코딩을 위한 inverse 사전 만들기\n","  inverse_vocab = dict(enumerate(vocabulary))\n","  # 디코딩할 특정 날짜 속보 읽어오기\n","  df_test = get_oneday_sokbo_from_db(sdate)\n","  # 해당 날짜 모든 뉴스를 디코딩해보기\n","  for idx in range(len(df_test)):\n","    # 원문 텍스트 출력\n","    print(df_test.iloc[idx]['summary'])\n","    # 정수 인덱스 리스트 저장된 '2000_encoded_text_code' 컬럼을 디코딩하기\n","    decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in re.findall(r'\\d+', df_test.at[idx,'20000_encoded_text_code']))\n","    # 디코드된 결과 출력\n","    print(decoded_sentence)\n","\n","# -- decode_today_sokbo_20000() END:--------------------------------------------\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":47052,"status":"ok","timestamp":1707743624924,"user":{"displayName":"hyun","userId":"14298231370728061195"},"user_tz":-540},"id":"D__d--HuMRXF","outputId":"3455c0ab-b4f8-4aa2-f210-760f1e0e8138"},"outputs":[],"source":["#================================================================\n","# --메인 함수 호출 -----------------------------------------------\n","get_today_news_sokbo()          # 오늘 속보 웹크롤링\n","tokenize_today_news_summary()   # 오늘 속보 토큰화\n","encode_save_today_sokbo_20000() # 오늘 속보 벡터화 인코딩\n","# ---------------------------------------------------------------\n","decode_today_sokbo_20000()      # 오늘 속보 디코딩 확인\n","# --메인 함수 호출 :END-------------------------------------------\n","#================================================================"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPqaXrC7kPxj1njciq+8gH+","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
