{"cells":[{"cell_type":"markdown","metadata":{"id":"GOewi6Eull5c"},"source":["1. 네이버증권 뉴스 본문 DB에서 가져와서 토큰화, 벡터화 하기\n","- 테이블 분리 10등분\n","- DB에서 본문 텍스트 summary 읽어오기\n","- konlpy로 토큰화 하기\n","- 이미 생성한 2만 단어 사전 으로 백터화\n","##* 코랩실행시 우선 konlpy 종목명으로 업데이트하기"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29695,"status":"ok","timestamp":1708208883925,"user":{"displayName":"hyun","userId":"14298231370728061195"},"user_tz":-540},"id":"mGlr8RD6iGl9","outputId":"425db1eb-0f6d-4eae-a69a-26cb715525f7"},"outputs":[],"source":["# 코랩에서 실행시 pyMySQL을 먼저 설치해야 한다\n","# !pip install JPype1\n","# !pip install konlpy\n","# !pip install pyMySQL\n","# !pip install -U finance-datareader"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":12243,"status":"ok","timestamp":1708228908780,"user":{"displayName":"hyun","userId":"14298231370728061195"},"user_tz":-540},"id":"jcMXtef2m6t1"},"outputs":[],"source":["# import packages\n","import os\n","import numpy as np\n","import pandas as pd\n","import FinanceDataReader as fdr\n","import konlpy\n","from konlpy.tag import Okt\n","import pymysql\n","from os import replace\n","import requests\n","from tqdm.notebook import tqdm\n","from sqlalchemy import create_engine\n","from datetime import datetime, timedelta\n","import ast\n","import pickle\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.layers import TextVectorization\n","import re\n","import string\n","# 리스트 flatten을 위한 itertools 패키지 import\n","import itertools\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"3B9T8H9VmnWd"},"source":["2. konlpy에 종목명과 약명을 새로운 단어로 추가하기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i-1p8EJCifIO"},"outputs":[],"source":["# --konlpy 커스터마이징  ----------------------------------------------\n","# --------------------------------------------------------------------\n","# Konlpy에 한국거래소 상장종목 전체 종목명과 약명을 새로운 단어로 추가하기\n","# --------------------------------------------------------------------\n","# konlpy 디렉토리로 이동\n","os.chdir('konlpy/설치된/디렉토리/java')\n","# 현재 디렉토리 확인\n","os.getcwd()\n","# 압축 풀 임시 디렉토리 /temp 만들기\n","os.makedirs('./temp')\n","# 임시디렉토리로 이동\n","os.chdir('./temp')\n","# 압축 풀기\n","!jar xvf ../open-korean-text-2.1.0.jar\n","\n","# FinanceDataReader로 한국거래소 상장종목 전체 종목을 가져오기\n","df_krx = fdr.StockListing('KRX')\n","# 종목명만 추출하기\n","name_list = df_krx['Name']\n","# 새로운 단어목록 생성을 위한 data 변수 선언\n","data = ''\n","# 종목명을 한줄씩(\\n)씩 넣어서 목록 만들기\n","for one_name in name_list:\n","  data += one_name+'\\n'\n","# 종목명 약명 새로운 단어로 넣기\n","# DB 연결 준비: 종목명 약어 읽어오기\n","conn = pymysql.connect(host= '호스트주소', port = 포트번호, user=\"아이디\", password=\"패스워드\", db=\"DB이름\", charset = 'utf8')\n","# DB에서 종목병 약어 추출위한 sql 준비\n","sql = f\"SELECT x.kor_name,x.kor_abb FROM DB이름.테이블이름 x\"\n","# DB 검색결과를 dataframe에 저장\n","name_abb_df = pd.read_sql_query(sql, conn)\n","# 실행확인을 위한 화면 출력\n","print(name_abb_df.head(3))\n","# DB close\n","conn.close()\n","# 종목명 추가하기\n","name_list = name_abb_df['kor_name']\n","# 종목명을 한줄씩(\\n)씩 넣어서 목록 만들기\n","for one_name in name_list:\n","  data += one_name+'\\n'\n","# 종목명 약명 추가하기\n","name_list = name_abb_df['kor_abb']\n","# 종목명 약명을 한줄씩(\\n)씩 넣어서 목록 만들기\n","for one_name in name_list:\n","  data += one_name+'\\n'\n","# konlpy의 company_names.txt 사전으로 저장\n","with open(\"konlpy/설치된/디렉토리/java/temp/org/openkoreantext/processor/util/noun/company_names.txt\", 'w') as f:\n","    f.write(data)\n","# 임시 작업 디렉토리로 이동 확인\n","os.chdir('konlpy/설치된/디렉토리/java/temp')\n","# 다시 압축\n","!jar cvf ../open-korean-text-2.1.0.jar *\n","# 코랩 실행시 Restart session 실행\n","\n","# --konlpy 커스터마이징 END --------------------------------------------"]},{"cell_type":"markdown","metadata":{"id":"3UWhWGh2ttbK"},"source":["* DB 에서 뉴스 본문 summary 전체 읽어오기"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":23702,"status":"ok","timestamp":1708240836131,"user":{"displayName":"hyun","userId":"14298231370728061195"},"user_tz":-540},"id":"r8UP2fHKvel5"},"outputs":[],"source":["# DB 연결 준비: 뉴스 본문 읽어오기\n","conn = pymysql.connect(host= '호스트 주소', port = 포트번호, user=\"아이디\", password=\"패스워드\", db=\"DB이름\", charset = 'utf8')\n","# DB에서 뉴스(summary, press, rdate) 추출위한 sql 준비\n","sql = f\"SELECT x.article_id, x.r_time, x.summary FROM DB이름.테이블이름 x\"\n","# DB 검색결과를 dataframe에 저장\n","result_df = pd.read_sql_query(sql, conn)\n","# 실행확인을 위한 화면 출력\n","#print(result_df.head(3))\n","# DB close\n","conn.close()"]},{"cell_type":"markdown","metadata":{"id":"bd4z98dUnUf2"},"source":["3. 뉴스 본문 summary를 10등분해서 테이블 나눠서 DB 저장\n","- 테이블 나누기: 너무 크면 load 가 안됨\n","- Tokenize해서 DB에 저장하기\n","- konlpy의 okt를 사용하여 형태소 분석하고 tokenize한 후 DB에 저장\n","- DB: fast_news_url, 테이블명: bonmoon_1_10 ~ bonmoon_10_10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lmv0TzRJ0GYc"},"outputs":[],"source":["# 전체 데이터 수: 약 24만개\n","print(len(result_df))\n","# 날짜순으로 정렬\n","result_df= result_df.sort_values('r_time')\n","# 데이터프레임의 전체 행 수 구하기\n","num_rows = len(result_df)\n","# 전체를 10등분으로 나누기위한 행수 구하기\n","split_size = num_rows // 10\n","# 나눈 결과 저장을 위한 리스트\n","slices = []\n","# 데이터프레임을 10등분으로 나누기: 슬라이싱\n","for i in range(0, num_rows, split_size):\n","    slices.append(result_df.iloc[i:i+split_size])\n","# 10개의 테이블로 나눠서 DB에 저장\n","for i in range(10):\n","  # 임시 df\n","  temp_df = slices[i]\n","  # DB 저장을 위한 연결 준비\n","  db_connection_path = 'mysql+pymysql://아이디:패스워드@호스트주소:포트번호/DB이름'\n","  db_connection = create_engine(db_connection_path)\n","  # 데이터프레임을 DB로 저장, bonmoon_{번호}_10를 테이블 명으로, 이미 있으면 덮어쓰기(replace)\n","  temp_df.to_sql('bonmoon_'+str(i+1)+'_10',con=db_connection,if_exists='replace',index=False)\n","  # DB close\n","  db_connection.dispose()\n"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1708240968677,"user":{"displayName":"hyun","userId":"14298231370728061195"},"user_tz":-540},"id":"mxQys67SPaGD","outputId":"db63665e-4268-42a5-d880-01332a4a5c6d"},"outputs":[{"name":"stdout","output_type":"stream","text":["24003\n"]}],"source":["# 한개의 테이블에 약 2만4천개\n","print(len(slices[0]))"]},{"cell_type":"markdown","metadata":{"id":"dcO-L6aamc6w"},"source":["* 토큰화,인코딩,DB 저장"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":303,"status":"ok","timestamp":1708241787839,"user":{"displayName":"hyun","userId":"14298231370728061195"},"user_tz":-540},"id":"C13JEeo6yNR3"},"outputs":[],"source":["# -- custom_standardization_fn() --------------------------------------------\n","# custom_표준화 함수\n","def custom_standardization_fn(string_tensor):\n","    lowercase_string = string_tensor # 영문의 경우 여기서 모두 소문자 처리, 한글은 필요없음\n","    return tf.strings.regex_replace(\n","        # 문장부호 처리: %와 .는 살리고 나머지는 제외\n","        lowercase_string, f\"[{re.escape(string.punctuation.replace('%','').replace('.',''))}]\", \"\") \n","# -- custom_standardization_fn() :END----------------------------------------\n","\n","# -- custom_split_fn()-------------------------------------------------------\n","# custom_토큰화 함수\n","def custom_split_fn(string_tensor):\n","    return tf.strings.split(string_tensor)  # 한글의 경우 형태소 분석 처리 해야 한다\n","# -- custom_split_fn() :END--------------------------------------------------\n","\n","# --tokenize_save_bonmoon()--------------------------------------------------\n","# 토큰화 해서 DB 저장\n","def tokenize_save_bonmoon(bonmoon_df,part_num):\n","  # 원본 copy 생성\n","  result_df = bonmoon_df.copy()\n","  # konlpy의 okt 생성\n","  okt=Okt()\n","  # 실행확인을 위한 화면 출력\n","  print(part_num)\n","  \n","  # okt로 tokenize해서 형태소 품사 morph 추출하기 (pos 는 okt.pos, 명사는 okt.nouns)\n","  result_df['morphs_tokenized_summary'] = result_df['summary'].apply(okt.morphs)\n","  # DB 저장을 위해 문자열로 변환\n","  result_df['morphs_tokenized_summary'] = result_df['morphs_tokenized_summary'].astype(str)\n","\n","  # okt로 tokenize해서 형태소 pos 추출하기 (품사 전체는 okt.morphs, 명사는 okt.nouns)\n","  result_df['pos_tokenized_summary'] = result_df['summary'].apply(okt.pos)\n","  \n","  # DB 저장을 위해 문자열로 변환\n","  result_df['pos_tokenized_summary'] = result_df['pos_tokenized_summary'].astype(str)\n","\n","  # okt로 tokenize해서 명사 형태소 nouns 추출하기 (품사 전체는 okt.morphs)\n","  result_df['nouns_tokenized_summary'] = result_df['summary'].apply(okt.nouns)\n","  \n","  # DB 저장을 위해 문자열로 변환\n","  result_df['nouns_tokenized_summary'] = result_df['nouns_tokenized_summary'].astype(str)\n","\n","  # pos tokenized 에서 Modifier,Josa,Suffix,Punctuation,Foreigh 제외, 1 자리 단어 제외\n","  result_df['tokenized_summary'] = \"\"\n","  for i in range(len(result_df)):\n","    # 새로운 문자열 위한 리스트\n","    new_str = []\n","    # 튜플 형태로 꺼내면 (단어,품사) 형태이므로 뒤세 품사를 보고 맞는 단어들만 새로운 문자열로 만든다\n","    for (x,y) in ast.literal_eval(result_df['pos_tokenized_summary'][i]):\n","      # 관형사,조사,접두사,문장부호,한자및 기타기호 제외\n","      if (y!='Modifier')&(y!='Josa')&(y!='Suffix')&(y!='Punctuation')&(y!='Foreign'):\n","        # 한글자 짜리 제외\n","        if(len(str(x))>1):\n","          new_str.append(x)\n","    # 실행 확인을 위한 화면 출력\n","    #print(new_str)\n","\n","    # 토크나이즈된 문자열 데이터프레임에 넣기\n","    result_df['tokenized_summary'][i]=str(new_str)\n","\n","  # custom 표준화, 토큰화 함수 지정해서 TextVectoriztion 생성하기\n","  text_vectorization = TextVectorization(\n","      # max_tokens 지정\n","      max_tokens=20000\n","      # 정수 인덱스로 출력\n","    ,  output_mode=\"int\"\n","      # 표준화 함수를 custom_표준화 함수로 지정\n","    ,  standardize=custom_standardization_fn\n","      # 토큰화 함수를 custom_토큰화 함수로 지정\n","    ,  split=custom_split_fn\n","  )\n","  #-------------------------------------------------------------------\n","  # --- 2만단어사전 불러오기 load : start ---\n","  from_disk = pickle.load(open(\"사전경로/tv_layer_20000.pkl\", \"rb\"))\n","\n","  new_vectorizer = TextVectorization(\n","      # max_tokens 지정\n","      max_tokens=20000\n","      # 정수 인덱스로 출력\n","    ,  output_mode=\"int\"\n","      # 표준화 함수를 custom_표준화 함수로 지정\n","    ,  standardize=custom_standardization_fn\n","      # 토큰화 함수를 custom_토큰화 함수로 지정\n","    ,  split=custom_split_fn\n","    )\n","  new_vectorizer.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\n","  new_vectorizer.set_weights(from_disk['weights'])\n","  # --- 2만단어사전 불러오기 load : end ---\n","  \n","  # 인코딩하기\n","  # 위 생성한 백터라이즈를 하루치 속보에 적용 \n","  result_df['20000_encoded_text']= list(map(new_vectorizer,result_df['tokenized_summary']))\n","  # 하루에 여러 건 뉴스가 있으니 모든 뉴스에 적용 \n","  for i in range(len(result_df)):\n","    # 숫자로 인코드된 뉴스를 문자열로 만들어서 DB에 저장\n","    result_df.at[i,'20000_encoded_text_code']= str(new_vectorizer(result_df.at[i,'tokenized_summary']).numpy()).replace('\\n', '')\n","  # DB 저장하기 위한 컬럼정리\n","  result_df = result_df[['article_id','r_time','summary','pos_tokenized_summary','tokenized_summary','20000_encoded_text_code']]\n","  # DB 저장을 위한 연결 준비\n","  db_connection_path = 'mysql+pymysql://아이디:패스워드@호스트주소:포트번호/DB이름'\n","  db_connection = create_engine(db_connection_path)\n","  # 데이터프레임을 DB로 저장, 날짜 ddate_sokbo를 테이블 명으로, 이미 있으면 덮어쓰기(replace)\n","  result_df.to_sql('bonmoon_'+str(part_num)+'_10',con=db_connection,if_exists='replace',index=False)\n","  # DB close\n","  db_connection.dispose()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5987970,"status":"ok","timestamp":1708248578799,"user":{"displayName":"hyun","userId":"14298231370728061195"},"user_tz":-540},"id":"nOHL_bLu0v6p","outputId":"5291a9ad-b3aa-4cc2-a958-1fdba6510754"},"outputs":[],"source":["#====================================================\n","# 뉴스 본문 summary 인코딩하기: 메인 함수 호출\n","#----------------------------------------------------\n","# 10개 테이블 순차적으로 처리\n","for i in range(1,10):\n","  # 테이블 일련번호\n","  part_num = i+1\n","  # DB 연결 준비: 뉴스 본문 읽어오기(5개 테이블)\n","  conn = pymysql.connect(host= '호스트주소', port = 포트번호, user=\"아이디\", password=\"패스워드\", db=\"DB이름\", charset = 'utf8')\n","  # DB에서 뉴스(summary, press, rdate) 추출위한 sql 준비\n","  sql = f\"SELECT x.*  FROM DB이름.bonmoon_{part_num}_10 x\"\n","  # DB 검색결과를 dataframe에 저장\n","  part_df = pd.read_sql_query(sql, conn)\n","  # DB close\n","  conn.close()\n","  # 실행확인을 위한 화면 출력\n","  print(part_df.head(3))\n","  #== 메인함수 호출 =========================\n","  tokenize_save_bonmoon(part_df, part_num)\n","  #=========================================\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BIMXUZxOrk7g"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cO6dVqBqrkyW"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pKIf3B7nrklI"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8KLD8qtpcXrW"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPE8bLGQbhISztcDZmqSCwT","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
